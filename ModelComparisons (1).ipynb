{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ModelComparisons.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nfpsrp0qib9l"
      },
      "source": [
        "Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDG_XpOU5ehX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ef5ce79-2f13-415c-a663-23e86e6c3a5f"
      },
      "source": [
        "import statsmodels.api as sm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd2Y9vMuH4BM"
      },
      "source": [
        "data = datasets.load_diabetes()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7a6Y0YsIon-",
        "outputId": "989fbd97-6236-41cd-add6-2abfdaec09f8"
      },
      "source": [
        "data.feature_names\n",
        "print(data.DESCR)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. _diabetes_dataset:\n",
            "\n",
            "Diabetes dataset\n",
            "----------------\n",
            "\n",
            "Ten baseline variables, age, sex, body mass index, average blood\n",
            "pressure, and six blood serum measurements were obtained for each of n =\n",
            "442 diabetes patients, as well as the response of interest, a\n",
            "quantitative measure of disease progression one year after baseline.\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "  :Number of Instances: 442\n",
            "\n",
            "  :Number of Attributes: First 10 columns are numeric predictive values\n",
            "\n",
            "  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n",
            "\n",
            "  :Attribute Information:\n",
            "      - age     age in years\n",
            "      - sex\n",
            "      - bmi     body mass index\n",
            "      - bp      average blood pressure\n",
            "      - s1      tc, total serum cholesterol\n",
            "      - s2      ldl, low-density lipoproteins\n",
            "      - s3      hdl, high-density lipoproteins\n",
            "      - s4      tch, total cholesterol / HDL\n",
            "      - s5      ltg, possibly log of serum triglycerides level\n",
            "      - s6      glu, blood sugar level\n",
            "\n",
            "Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\n",
            "\n",
            "Source URL:\n",
            "https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n",
            "\n",
            "For more information see:\n",
            "Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n",
            "(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GN-x90AsK1wm"
      },
      "source": [
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "\n",
        "target = pd.DataFrame(data.target, columns=[\"MEDV\"])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "KQXg5U20MVEB",
        "outputId": "34266561-18fe-4444-efaf-91ebcdfcc606"
      },
      "source": [
        "df"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>bmi</th>\n",
              "      <th>bp</th>\n",
              "      <th>s1</th>\n",
              "      <th>s2</th>\n",
              "      <th>s3</th>\n",
              "      <th>s4</th>\n",
              "      <th>s5</th>\n",
              "      <th>s6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.038076</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>0.061696</td>\n",
              "      <td>0.021872</td>\n",
              "      <td>-0.044223</td>\n",
              "      <td>-0.034821</td>\n",
              "      <td>-0.043401</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>0.019908</td>\n",
              "      <td>-0.017646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.001882</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.051474</td>\n",
              "      <td>-0.026328</td>\n",
              "      <td>-0.008449</td>\n",
              "      <td>-0.019163</td>\n",
              "      <td>0.074412</td>\n",
              "      <td>-0.039493</td>\n",
              "      <td>-0.068330</td>\n",
              "      <td>-0.092204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.085299</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>0.044451</td>\n",
              "      <td>-0.005671</td>\n",
              "      <td>-0.045599</td>\n",
              "      <td>-0.034194</td>\n",
              "      <td>-0.032356</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>0.002864</td>\n",
              "      <td>-0.025930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.089063</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.011595</td>\n",
              "      <td>-0.036656</td>\n",
              "      <td>0.012191</td>\n",
              "      <td>0.024991</td>\n",
              "      <td>-0.036038</td>\n",
              "      <td>0.034309</td>\n",
              "      <td>0.022692</td>\n",
              "      <td>-0.009362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.005383</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.036385</td>\n",
              "      <td>0.021872</td>\n",
              "      <td>0.003935</td>\n",
              "      <td>0.015596</td>\n",
              "      <td>0.008142</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>-0.031991</td>\n",
              "      <td>-0.046641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>437</th>\n",
              "      <td>0.041708</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>0.019662</td>\n",
              "      <td>0.059744</td>\n",
              "      <td>-0.005697</td>\n",
              "      <td>-0.002566</td>\n",
              "      <td>-0.028674</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>0.031193</td>\n",
              "      <td>0.007207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>438</th>\n",
              "      <td>-0.005515</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>-0.015906</td>\n",
              "      <td>-0.067642</td>\n",
              "      <td>0.049341</td>\n",
              "      <td>0.079165</td>\n",
              "      <td>-0.028674</td>\n",
              "      <td>0.034309</td>\n",
              "      <td>-0.018118</td>\n",
              "      <td>0.044485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>439</th>\n",
              "      <td>0.041708</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>-0.015906</td>\n",
              "      <td>0.017282</td>\n",
              "      <td>-0.037344</td>\n",
              "      <td>-0.013840</td>\n",
              "      <td>-0.024993</td>\n",
              "      <td>-0.011080</td>\n",
              "      <td>-0.046879</td>\n",
              "      <td>0.015491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>440</th>\n",
              "      <td>-0.045472</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>0.039062</td>\n",
              "      <td>0.001215</td>\n",
              "      <td>0.016318</td>\n",
              "      <td>0.015283</td>\n",
              "      <td>-0.028674</td>\n",
              "      <td>0.026560</td>\n",
              "      <td>0.044528</td>\n",
              "      <td>-0.025930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>441</th>\n",
              "      <td>-0.045472</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.073030</td>\n",
              "      <td>-0.081414</td>\n",
              "      <td>0.083740</td>\n",
              "      <td>0.027809</td>\n",
              "      <td>0.173816</td>\n",
              "      <td>-0.039493</td>\n",
              "      <td>-0.004220</td>\n",
              "      <td>0.003064</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>442 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          age       sex       bmi  ...        s4        s5        s6\n",
              "0    0.038076  0.050680  0.061696  ... -0.002592  0.019908 -0.017646\n",
              "1   -0.001882 -0.044642 -0.051474  ... -0.039493 -0.068330 -0.092204\n",
              "2    0.085299  0.050680  0.044451  ... -0.002592  0.002864 -0.025930\n",
              "3   -0.089063 -0.044642 -0.011595  ...  0.034309  0.022692 -0.009362\n",
              "4    0.005383 -0.044642 -0.036385  ... -0.002592 -0.031991 -0.046641\n",
              "..        ...       ...       ...  ...       ...       ...       ...\n",
              "437  0.041708  0.050680  0.019662  ... -0.002592  0.031193  0.007207\n",
              "438 -0.005515  0.050680 -0.015906  ...  0.034309 -0.018118  0.044485\n",
              "439  0.041708  0.050680 -0.015906  ... -0.011080 -0.046879  0.015491\n",
              "440 -0.045472 -0.044642  0.039062  ...  0.026560  0.044528 -0.025930\n",
              "441 -0.045472 -0.044642 -0.073030  ... -0.039493 -0.004220  0.003064\n",
              "\n",
              "[442 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "Xgypyp_AMXzx",
        "outputId": "3ecbc939-d825-4900-bb0b-03c1067e9051"
      },
      "source": [
        "target"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MEDV</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>151.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>75.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>141.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>206.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>135.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>437</th>\n",
              "      <td>178.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>438</th>\n",
              "      <td>104.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>439</th>\n",
              "      <td>132.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>440</th>\n",
              "      <td>220.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>441</th>\n",
              "      <td>57.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>442 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      MEDV\n",
              "0    151.0\n",
              "1     75.0\n",
              "2    141.0\n",
              "3    206.0\n",
              "4    135.0\n",
              "..     ...\n",
              "437  178.0\n",
              "438  104.0\n",
              "439  132.0\n",
              "440  220.0\n",
              "441   57.0\n",
              "\n",
              "[442 rows x 1 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "TKCI-jB4Mag5",
        "outputId": "c1e387f7-0733-4952-ad41-7b1270cf28a1"
      },
      "source": [
        "X = df[\"bmi\"]\n",
        "y = target[\"MEDV\"]\n",
        "\n",
        "model = sm.OLS(y, X).fit()\n",
        "predictions = model.predict(X) \n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>OLS Regression Results</caption>\n",
              "<tr>\n",
              "  <th>Dep. Variable:</th>          <td>MEDV</td>       <th>  R-squared (uncentered):</th>      <td>   0.070</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared (uncentered):</th> <td>   0.068</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>          <td>   33.27</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Date:</th>             <td>Tue, 30 Nov 2021</td> <th>  Prob (F-statistic):</th>          <td>1.51e-08</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Time:</th>                 <td>20:57:38</td>     <th>  Log-Likelihood:    </th>          <td> -2882.5</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>No. Observations:</th>      <td>   442</td>      <th>  AIC:               </th>          <td>   5767.</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Residuals:</th>          <td>   441</td>      <th>  BIC:               </th>          <td>   5771.</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>              <td> </td>   \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>              <td> </td>   \n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "   <td></td>      <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>bmi</th> <td>  949.4353</td> <td>  164.610</td> <td>    5.768</td> <td> 0.000</td> <td>  625.918</td> <td> 1272.952</td>\n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "  <th>Omnibus:</th>       <td>11.674</td> <th>  Durbin-Watson:     </th> <td>   0.266</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Prob(Omnibus):</th> <td> 0.003</td> <th>  Jarque-Bera (JB):  </th> <td>   7.310</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Skew:</th>          <td> 0.156</td> <th>  Prob(JB):          </th> <td>  0.0259</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Kurtosis:</th>      <td> 2.453</td> <th>  Cond. No.          </th> <td>    1.00</td>\n",
              "</tr>\n",
              "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
            ],
            "text/plain": [
              "<class 'statsmodels.iolib.summary.Summary'>\n",
              "\"\"\"\n",
              "                                 OLS Regression Results                                \n",
              "=======================================================================================\n",
              "Dep. Variable:                   MEDV   R-squared (uncentered):                   0.070\n",
              "Model:                            OLS   Adj. R-squared (uncentered):              0.068\n",
              "Method:                 Least Squares   F-statistic:                              33.27\n",
              "Date:                Tue, 30 Nov 2021   Prob (F-statistic):                    1.51e-08\n",
              "Time:                        20:57:38   Log-Likelihood:                         -2882.5\n",
              "No. Observations:                 442   AIC:                                      5767.\n",
              "Df Residuals:                     441   BIC:                                      5771.\n",
              "Df Model:                           1                                                  \n",
              "Covariance Type:            nonrobust                                                  \n",
              "==============================================================================\n",
              "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
              "------------------------------------------------------------------------------\n",
              "bmi          949.4353    164.610      5.768      0.000     625.918    1272.952\n",
              "==============================================================================\n",
              "Omnibus:                       11.674   Durbin-Watson:                   0.266\n",
              "Prob(Omnibus):                  0.003   Jarque-Bera (JB):                7.310\n",
              "Skew:                           0.156   Prob(JB):                       0.0259\n",
              "Kurtosis:                       2.453   Cond. No.                         1.00\n",
              "==============================================================================\n",
              "\n",
              "Warnings:\n",
              "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
              "\"\"\""
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "DAlgoSoxNy_p",
        "outputId": "cbbdb5eb-20c8-43ff-d5c9-407860cb56e1"
      },
      "source": [
        "X = df[\"bmi\"] \n",
        "y = target[\"MEDV\"] \n",
        "X = sm.add_constant(X) \n",
        "\n",
        "model = sm.OLS(y, X).fit() \n",
        "predictions = model.predict(X)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>OLS Regression Results</caption>\n",
              "<tr>\n",
              "  <th>Dep. Variable:</th>          <td>MEDV</td>       <th>  R-squared:         </th> <td>   0.344</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.342</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   230.7</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Date:</th>             <td>Tue, 30 Nov 2021</td> <th>  Prob (F-statistic):</th> <td>3.47e-42</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Time:</th>                 <td>20:57:42</td>     <th>  Log-Likelihood:    </th> <td> -2454.0</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>No. Observations:</th>      <td>   442</td>      <th>  AIC:               </th> <td>   4912.</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Residuals:</th>          <td>   440</td>      <th>  BIC:               </th> <td>   4920.</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>const</th> <td>  152.1335</td> <td>    2.974</td> <td>   51.162</td> <td> 0.000</td> <td>  146.289</td> <td>  157.978</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>bmi</th>   <td>  949.4353</td> <td>   62.515</td> <td>   15.187</td> <td> 0.000</td> <td>  826.570</td> <td> 1072.301</td>\n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "  <th>Omnibus:</th>       <td>11.674</td> <th>  Durbin-Watson:     </th> <td>   1.848</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Prob(Omnibus):</th> <td> 0.003</td> <th>  Jarque-Bera (JB):  </th> <td>   7.310</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Skew:</th>          <td> 0.156</td> <th>  Prob(JB):          </th> <td>  0.0259</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Kurtosis:</th>      <td> 2.453</td> <th>  Cond. No.          </th> <td>    21.0</td>\n",
              "</tr>\n",
              "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
            ],
            "text/plain": [
              "<class 'statsmodels.iolib.summary.Summary'>\n",
              "\"\"\"\n",
              "                            OLS Regression Results                            \n",
              "==============================================================================\n",
              "Dep. Variable:                   MEDV   R-squared:                       0.344\n",
              "Model:                            OLS   Adj. R-squared:                  0.342\n",
              "Method:                 Least Squares   F-statistic:                     230.7\n",
              "Date:                Tue, 30 Nov 2021   Prob (F-statistic):           3.47e-42\n",
              "Time:                        20:57:42   Log-Likelihood:                -2454.0\n",
              "No. Observations:                 442   AIC:                             4912.\n",
              "Df Residuals:                     440   BIC:                             4920.\n",
              "Df Model:                           1                                         \n",
              "Covariance Type:            nonrobust                                         \n",
              "==============================================================================\n",
              "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
              "------------------------------------------------------------------------------\n",
              "const        152.1335      2.974     51.162      0.000     146.289     157.978\n",
              "bmi          949.4353     62.515     15.187      0.000     826.570    1072.301\n",
              "==============================================================================\n",
              "Omnibus:                       11.674   Durbin-Watson:                   1.848\n",
              "Prob(Omnibus):                  0.003   Jarque-Bera (JB):                7.310\n",
              "Skew:                           0.156   Prob(JB):                       0.0259\n",
              "Kurtosis:                       2.453   Cond. No.                         21.0\n",
              "==============================================================================\n",
              "\n",
              "Warnings:\n",
              "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
              "\"\"\""
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "-FS6sG1UOIRV",
        "outputId": "925802a6-477b-4ef7-9ebc-c72833af2dc4"
      },
      "source": [
        "X = df[[\"bmi\", \"bp\"]]\n",
        "y = target[\"MEDV\"]\n",
        "model = sm.OLS(y, X).fit()\n",
        "predictions = model.predict(X)\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>OLS Regression Results</caption>\n",
              "<tr>\n",
              "  <th>Dep. Variable:</th>          <td>MEDV</td>       <th>  R-squared (uncentered):</th>      <td>   0.081</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared (uncentered):</th> <td>   0.077</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>          <td>   19.33</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Date:</th>             <td>Tue, 30 Nov 2021</td> <th>  Prob (F-statistic):</th>          <td>8.99e-09</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Time:</th>                 <td>20:57:45</td>     <th>  Log-Likelihood:    </th>          <td> -2879.9</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>No. Observations:</th>      <td>   442</td>      <th>  AIC:               </th>          <td>   5764.</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Residuals:</th>          <td>   440</td>      <th>  BIC:               </th>          <td>   5772.</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>              <td> </td>   \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>              <td> </td>   \n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "   <td></td>      <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>bmi</th> <td>  790.3966</td> <td>  178.391</td> <td>    4.431</td> <td> 0.000</td> <td>  439.792</td> <td> 1141.002</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>bp</th>  <td>  402.2067</td> <td>  178.391</td> <td>    2.255</td> <td> 0.025</td> <td>   51.602</td> <td>  752.812</td>\n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "  <th>Omnibus:</th>       <td>12.428</td> <th>  Durbin-Watson:     </th> <td>   0.259</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Prob(Omnibus):</th> <td> 0.002</td> <th>  Jarque-Bera (JB):  </th> <td>   7.822</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Skew:</th>          <td> 0.171</td> <th>  Prob(JB):          </th> <td>  0.0200</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Kurtosis:</th>      <td> 2.446</td> <th>  Cond. No.          </th> <td>    1.52</td>\n",
              "</tr>\n",
              "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
            ],
            "text/plain": [
              "<class 'statsmodels.iolib.summary.Summary'>\n",
              "\"\"\"\n",
              "                                 OLS Regression Results                                \n",
              "=======================================================================================\n",
              "Dep. Variable:                   MEDV   R-squared (uncentered):                   0.081\n",
              "Model:                            OLS   Adj. R-squared (uncentered):              0.077\n",
              "Method:                 Least Squares   F-statistic:                              19.33\n",
              "Date:                Tue, 30 Nov 2021   Prob (F-statistic):                    8.99e-09\n",
              "Time:                        20:57:45   Log-Likelihood:                         -2879.9\n",
              "No. Observations:                 442   AIC:                                      5764.\n",
              "Df Residuals:                     440   BIC:                                      5772.\n",
              "Df Model:                           2                                                  \n",
              "Covariance Type:            nonrobust                                                  \n",
              "==============================================================================\n",
              "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
              "------------------------------------------------------------------------------\n",
              "bmi          790.3966    178.391      4.431      0.000     439.792    1141.002\n",
              "bp           402.2067    178.391      2.255      0.025      51.602     752.812\n",
              "==============================================================================\n",
              "Omnibus:                       12.428   Durbin-Watson:                   0.259\n",
              "Prob(Omnibus):                  0.002   Jarque-Bera (JB):                7.822\n",
              "Skew:                           0.171   Prob(JB):                       0.0200\n",
              "Kurtosis:                       2.446   Cond. No.                         1.52\n",
              "==============================================================================\n",
              "\n",
              "Warnings:\n",
              "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
              "\"\"\""
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aDlajISOdda"
      },
      "source": [
        "from sklearn import linear_model"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaAGFz06OfcV"
      },
      "source": [
        "from sklearn import datasets\n",
        "data = datasets.load_diabetes() "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0cVORwWOmXV"
      },
      "source": [
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "\n",
        "target = pd.DataFrame(data.target, columns=[\"MEDV\"])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tr3xlG9jOufN"
      },
      "source": [
        "X = df\n",
        "y = target[\"MEDV\"]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGWet3jFOvpc"
      },
      "source": [
        "lm = linear_model.LinearRegression()\n",
        "model = lm.fit(X,y)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Nn_c0NYO0Sl",
        "outputId": "c74be3dc-4c45-4504-d1d0-63eadc41e525"
      },
      "source": [
        "predictions = lm.predict(X)\n",
        "print(predictions[0:5])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[206.11706979  68.07234761 176.88406035 166.91796559 128.45984241]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIJU26RSO3wZ",
        "outputId": "f4ff6938-4eed-457e-e59f-28ddcb7ec09a"
      },
      "source": [
        "from sklearn.metrics import r2_score,mean_squared_error\n",
        "print(r2_score(predictions,y),mean_squared_error(predictions,y))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.06856376672605669 2859.6903987680657\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIWr4npZPDN4",
        "outputId": "c6b1ab4d-6893-49c0-d4f8-07f107bade40"
      },
      "source": [
        "data.feature_names"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIiGJaNqPJ-Q",
        "outputId": "9f90b7c9-92de-4d63-80ca-c12885dfc798"
      },
      "source": [
        "lm.coef_"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ -10.01219782, -239.81908937,  519.83978679,  324.39042769,\n",
              "       -792.18416163,  476.74583782,  101.04457032,  177.06417623,\n",
              "        751.27932109,   67.62538639])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLRAWjsnPP_1",
        "outputId": "4a4ea314-4963-429e-fe60-93d86b1cd49b"
      },
      "source": [
        "lm.intercept_"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "152.1334841628965"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e33nqSmAPSIL"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "xyVw2wwCPUkB",
        "outputId": "878aed99-9549-45a2-b436-c32cc2a13bdf"
      },
      "source": [
        "plt.scatter(predictions, predictions-y, c='g', s = 40)\n",
        "plt.hlines(y=0, xmin=0, xmax=50)\n",
        "plt.title('Residual plot')\n",
        "plt.ylabel('Residual')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Residual')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36pPKyXhnvpN"
      },
      "source": [
        "X Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OStwKz_nxgc"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn import datasets, linear_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqF1b0M-n0V6"
      },
      "source": [
        "columns = \"age sex bmi map tc ldl hdl tch ltg glu\".split()\n",
        "diabetes = datasets.load_diabetes() \n",
        "df = pd.DataFrame(diabetes.data, columns=columns) \n",
        "y = diabetes.target "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivO1ITWon8Fo",
        "outputId": "f81860d6-f684-4f16-ca2e-376fbbb04a94"
      },
      "source": [
        "print(df)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          age       sex       bmi  ...       tch       ltg       glu\n",
            "0    0.038076  0.050680  0.061696  ... -0.002592  0.019908 -0.017646\n",
            "1   -0.001882 -0.044642 -0.051474  ... -0.039493 -0.068330 -0.092204\n",
            "2    0.085299  0.050680  0.044451  ... -0.002592  0.002864 -0.025930\n",
            "3   -0.089063 -0.044642 -0.011595  ...  0.034309  0.022692 -0.009362\n",
            "4    0.005383 -0.044642 -0.036385  ... -0.002592 -0.031991 -0.046641\n",
            "..        ...       ...       ...  ...       ...       ...       ...\n",
            "437  0.041708  0.050680  0.019662  ... -0.002592  0.031193  0.007207\n",
            "438 -0.005515  0.050680 -0.015906  ...  0.034309 -0.018118  0.044485\n",
            "439  0.041708  0.050680 -0.015906  ... -0.011080 -0.046879  0.015491\n",
            "440 -0.045472 -0.044642  0.039062  ...  0.026560  0.044528 -0.025930\n",
            "441 -0.045472 -0.044642 -0.073030  ... -0.039493 -0.004220  0.003064\n",
            "\n",
            "[442 rows x 10 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6Bw8bqyn-qQ",
        "outputId": "4817fba9-1087-455f-ea38-37a44d16b9a2"
      },
      "source": [
        "# create training and testing vars\n",
        "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2)\n",
        "print (X_train.shape, y_train.shape)\n",
        "print (X_test.shape, y_test.shape)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(353, 10) (353,)\n",
            "(89, 10) (89,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ws-Fgd_5oCCt"
      },
      "source": [
        "lm = linear_model.LinearRegression()\n",
        "model = lm.fit(X_train, y_train)\n",
        "predictions = lm.predict(X_test)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-dOJEu3oEbA",
        "outputId": "cd90d866-ef14-4816-f9e9-463dd5b76c02"
      },
      "source": [
        "print(predictions[0:5])\n",
        "print(y_test[0:5])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[134.52476534 216.08273286 101.45414731 113.04122771 161.64544748]\n",
            "[111.  77.  93.  74. 160.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "9MhqrG5qoHTH",
        "outputId": "ec54bc40-695c-4bb4-ea3c-0a706fafafb8"
      },
      "source": [
        "plt.scatter(y_test, predictions)\n",
        "plt.xlabel(\"True Values\")\n",
        "plt.ylabel(\"Predictions\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Predictions')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ1PPdI_oKZI",
        "outputId": "c2ede3f0-37c2-46dc-d011-366987410f12"
      },
      "source": [
        "print (\"Score:\", model.score(X_test, y_test))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: 0.45400338713586363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBNxpsOsoND4",
        "outputId": "a128d1b4-c24d-4785-ce57-d2d6bd984771"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import KFold \n",
        "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]]) \n",
        "y = np.array([1, 2, 3, 4]) \n",
        "kf = KFold(n_splits=2) \n",
        "kf.get_n_splits(X) \n",
        "print(kf) \n",
        "KFold(n_splits=2, random_state=None, shuffle=False)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KFold(n_splits=2, random_state=None, shuffle=False)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KFold(n_splits=2, random_state=None, shuffle=False)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yR5CeEOBopog",
        "outputId": "07c03dd6-c629-4181-a27c-20071b92646b"
      },
      "source": [
        "for train_index, test_index in kf.split(X):\n",
        " print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        " X_train, X_test = X[train_index], X[test_index]\n",
        " y_train, y_test = y[train_index], y[test_index]\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [2 3] TEST: [0 1]\n",
            "TRAIN: [0 1] TEST: [2 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0HR7pY8osjp",
        "outputId": "311ac251-846b-46fc-ef0b-e9d63cdb555c"
      },
      "source": [
        "from sklearn.model_selection import LeaveOneOut \n",
        "X = np.array([[1, 2], [3, 4]])\n",
        "y = np.array([1, 2])\n",
        "loo = LeaveOneOut()\n",
        "loo.get_n_splits(X)\n",
        "\n",
        "\n",
        "for train_index, test_index in loo.split(X):\n",
        "   print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "   X_train, X_test = X[train_index], X[test_index]\n",
        "   y_train, y_test = y[train_index], y[test_index]\n",
        "   print(X_train, X_test, y_train, y_test)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [1] TEST: [0]\n",
            "[[3 4]] [[1 2]] [2] [1]\n",
            "TRAIN: [0] TEST: [1]\n",
            "[[1 2]] [[3 4]] [1] [2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuWHrFPUot5X",
        "outputId": "e7f665a9-1510-4fa2-c760-6f20dd99d3f2"
      },
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn import datasets, linear_model\n",
        "\n",
        "columns = \"age sex bmi map tc ldl hdl tch ltg glu\".split() \n",
        "diabetes = datasets.load_diabetes() \n",
        "X = pd.DataFrame(diabetes.data, columns=columns) \n",
        "y = diabetes.target \n",
        "\n",
        "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
        "\n",
        "model = linear_model.LinearRegression()\n",
        "\n",
        "scores = cross_val_score(model, X, y , cv=cv, n_jobs=-1)\n",
        "\n",
        "print('Score: %.3f (%.3f)' % (mean(scores), std(scores)))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: 0.470 (0.090)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--3Rx-69pBLB",
        "outputId": "f1b7320b-1f32-420a-9314-390ff76849cf"
      },
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn import datasets, linear_model\n",
        "\n",
        "columns = \"age sex bmi map tc ldl hdl tch ltg glu\".split() \n",
        "diabetes = datasets.load_diabetes() \n",
        "X = pd.DataFrame(diabetes.data, columns=columns)\n",
        "y = diabetes.target \n",
        "\n",
        "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\n",
        "model = linear_model.LinearRegression()\n",
        "\n",
        "scores = cross_val_score(model, X, y, cv=cv, n_jobs=-1)\n",
        "\n",
        "print('Score: %.3f (%.3f)' % (mean(scores), std(scores)))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: 0.472 (0.124)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 812
        },
        "id": "KLRxGU8wpScw",
        "outputId": "0dbd58b8-e74e-4047-b5d1-0c6a70965949"
      },
      "source": [
        "from scipy.stats import sem\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn import datasets, linear_model\n",
        "from matplotlib import pyplot\n",
        "\n",
        "def evaluate_model(X, y, repeats):\n",
        "\t\n",
        "\tcv = RepeatedKFold(n_splits=10, n_repeats=repeats, random_state=1)\n",
        "\t\n",
        "\tmodel = linear_model.LinearRegression()\n",
        "\t\n",
        "\tscores = cross_val_score(model, X, y,  cv=cv, n_jobs=-1)\n",
        "\treturn scores\n",
        "\n",
        "columns = \"age sex bmi map tc ldl hdl tch ltg glu\".split()\n",
        "diabetes = datasets.load_diabetes() \n",
        "X = pd.DataFrame(diabetes.data, columns=columns) \n",
        "y = diabetes.target \n",
        "\n",
        "repeats = range(1,30)\n",
        "results = list()\n",
        "for r in repeats:\n",
        "\t\n",
        "\tscores = evaluate_model(X, y, r)\n",
        "\t\n",
        "\tprint('-%d mean=%.4f se=%.3f' % (r, mean(scores), sem(scores)))\n",
        "\t\n",
        "\tresults.append(scores)\n",
        "\n",
        "pyplot.boxplot(results, labels=[str(r) for r in repeats], showmeans=True)\n",
        "pyplot.show()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-1 mean=0.4703 se=0.030\n",
            "-2 mean=0.4700 se=0.025\n",
            "-3 mean=0.4717 se=0.023\n",
            "-4 mean=0.4733 se=0.018\n",
            "-5 mean=0.4766 se=0.016\n",
            "-6 mean=0.4750 se=0.015\n",
            "-7 mean=0.4732 se=0.014\n",
            "-8 mean=0.4735 se=0.012\n",
            "-9 mean=0.4709 se=0.012\n",
            "-10 mean=0.4701 se=0.011\n",
            "-11 mean=0.4699 se=0.011\n",
            "-12 mean=0.4688 se=0.011\n",
            "-13 mean=0.4692 se=0.010\n",
            "-14 mean=0.4694 se=0.010\n",
            "-15 mean=0.4694 se=0.010\n",
            "-16 mean=0.4706 se=0.009\n",
            "-17 mean=0.4713 se=0.009\n",
            "-18 mean=0.4725 se=0.009\n",
            "-19 mean=0.4729 se=0.008\n",
            "-20 mean=0.4721 se=0.008\n",
            "-21 mean=0.4725 se=0.008\n",
            "-22 mean=0.4716 se=0.008\n",
            "-23 mean=0.4720 se=0.008\n",
            "-24 mean=0.4723 se=0.007\n",
            "-25 mean=0.4731 se=0.007\n",
            "-26 mean=0.4724 se=0.007\n",
            "-27 mean=0.4727 se=0.007\n",
            "-28 mean=0.4727 se=0.007\n",
            "-29 mean=0.4722 se=0.007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EhvWpcTNSz5"
      },
      "source": [
        "Forward Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8I1bOb9JNYDv"
      },
      "source": [
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import pandas.testing as tm\n",
        "import numpy as np\n",
        "import itertools\n",
        "import time\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "d7xFf6hv6h5I",
        "outputId": "3baca086-992c-4049-f873-148667499643"
      },
      "source": [
        "mid_df =pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/midwest.csv')\n",
        "mid_df.head()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PID</th>\n",
              "      <th>county</th>\n",
              "      <th>state</th>\n",
              "      <th>area</th>\n",
              "      <th>poptotal</th>\n",
              "      <th>popdensity</th>\n",
              "      <th>popwhite</th>\n",
              "      <th>popblack</th>\n",
              "      <th>popamerindian</th>\n",
              "      <th>popasian</th>\n",
              "      <th>popother</th>\n",
              "      <th>percwhite</th>\n",
              "      <th>percblack</th>\n",
              "      <th>percamerindan</th>\n",
              "      <th>percasian</th>\n",
              "      <th>percother</th>\n",
              "      <th>popadults</th>\n",
              "      <th>perchsd</th>\n",
              "      <th>percollege</th>\n",
              "      <th>percprof</th>\n",
              "      <th>poppovertyknown</th>\n",
              "      <th>percpovertyknown</th>\n",
              "      <th>percbelowpoverty</th>\n",
              "      <th>percchildbelowpovert</th>\n",
              "      <th>percadultpoverty</th>\n",
              "      <th>percelderlypoverty</th>\n",
              "      <th>inmetro</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>561</td>\n",
              "      <td>ADAMS</td>\n",
              "      <td>IL</td>\n",
              "      <td>0.052</td>\n",
              "      <td>66090</td>\n",
              "      <td>1270.961540</td>\n",
              "      <td>63917</td>\n",
              "      <td>1702</td>\n",
              "      <td>98</td>\n",
              "      <td>249</td>\n",
              "      <td>124</td>\n",
              "      <td>96.712059</td>\n",
              "      <td>2.575276</td>\n",
              "      <td>0.148283</td>\n",
              "      <td>0.376759</td>\n",
              "      <td>0.187623</td>\n",
              "      <td>43298</td>\n",
              "      <td>75.107395</td>\n",
              "      <td>19.631392</td>\n",
              "      <td>4.355859</td>\n",
              "      <td>63628</td>\n",
              "      <td>96.274777</td>\n",
              "      <td>13.151443</td>\n",
              "      <td>18.011717</td>\n",
              "      <td>11.009776</td>\n",
              "      <td>12.443812</td>\n",
              "      <td>0</td>\n",
              "      <td>AAR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>562</td>\n",
              "      <td>ALEXANDER</td>\n",
              "      <td>IL</td>\n",
              "      <td>0.014</td>\n",
              "      <td>10626</td>\n",
              "      <td>759.000000</td>\n",
              "      <td>7054</td>\n",
              "      <td>3496</td>\n",
              "      <td>19</td>\n",
              "      <td>48</td>\n",
              "      <td>9</td>\n",
              "      <td>66.384340</td>\n",
              "      <td>32.900433</td>\n",
              "      <td>0.178807</td>\n",
              "      <td>0.451722</td>\n",
              "      <td>0.084698</td>\n",
              "      <td>6724</td>\n",
              "      <td>59.726353</td>\n",
              "      <td>11.243308</td>\n",
              "      <td>2.870315</td>\n",
              "      <td>10529</td>\n",
              "      <td>99.087145</td>\n",
              "      <td>32.244278</td>\n",
              "      <td>45.826514</td>\n",
              "      <td>27.385647</td>\n",
              "      <td>25.228976</td>\n",
              "      <td>0</td>\n",
              "      <td>LHR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>563</td>\n",
              "      <td>BOND</td>\n",
              "      <td>IL</td>\n",
              "      <td>0.022</td>\n",
              "      <td>14991</td>\n",
              "      <td>681.409091</td>\n",
              "      <td>14477</td>\n",
              "      <td>429</td>\n",
              "      <td>35</td>\n",
              "      <td>16</td>\n",
              "      <td>34</td>\n",
              "      <td>96.571276</td>\n",
              "      <td>2.861717</td>\n",
              "      <td>0.233473</td>\n",
              "      <td>0.106731</td>\n",
              "      <td>0.226803</td>\n",
              "      <td>9669</td>\n",
              "      <td>69.334988</td>\n",
              "      <td>17.033819</td>\n",
              "      <td>4.488572</td>\n",
              "      <td>14235</td>\n",
              "      <td>94.956974</td>\n",
              "      <td>12.068844</td>\n",
              "      <td>14.036061</td>\n",
              "      <td>10.852090</td>\n",
              "      <td>12.697410</td>\n",
              "      <td>0</td>\n",
              "      <td>AAR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>564</td>\n",
              "      <td>BOONE</td>\n",
              "      <td>IL</td>\n",
              "      <td>0.017</td>\n",
              "      <td>30806</td>\n",
              "      <td>1812.117650</td>\n",
              "      <td>29344</td>\n",
              "      <td>127</td>\n",
              "      <td>46</td>\n",
              "      <td>150</td>\n",
              "      <td>1139</td>\n",
              "      <td>95.254171</td>\n",
              "      <td>0.412257</td>\n",
              "      <td>0.149322</td>\n",
              "      <td>0.486918</td>\n",
              "      <td>3.697332</td>\n",
              "      <td>19272</td>\n",
              "      <td>75.472188</td>\n",
              "      <td>17.278954</td>\n",
              "      <td>4.197800</td>\n",
              "      <td>30337</td>\n",
              "      <td>98.477569</td>\n",
              "      <td>7.209019</td>\n",
              "      <td>11.179536</td>\n",
              "      <td>5.536013</td>\n",
              "      <td>6.217047</td>\n",
              "      <td>1</td>\n",
              "      <td>ALU</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>565</td>\n",
              "      <td>BROWN</td>\n",
              "      <td>IL</td>\n",
              "      <td>0.018</td>\n",
              "      <td>5836</td>\n",
              "      <td>324.222222</td>\n",
              "      <td>5264</td>\n",
              "      <td>547</td>\n",
              "      <td>14</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>90.198766</td>\n",
              "      <td>9.372858</td>\n",
              "      <td>0.239890</td>\n",
              "      <td>0.085675</td>\n",
              "      <td>0.102810</td>\n",
              "      <td>3979</td>\n",
              "      <td>68.861523</td>\n",
              "      <td>14.475999</td>\n",
              "      <td>3.367680</td>\n",
              "      <td>4815</td>\n",
              "      <td>82.505140</td>\n",
              "      <td>13.520249</td>\n",
              "      <td>13.022889</td>\n",
              "      <td>11.143211</td>\n",
              "      <td>19.200000</td>\n",
              "      <td>0</td>\n",
              "      <td>AAR</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PID     county state  ...  percelderlypoverty  inmetro  category\n",
              "0  561      ADAMS    IL  ...           12.443812        0       AAR\n",
              "1  562  ALEXANDER    IL  ...           25.228976        0       LHR\n",
              "2  563       BOND    IL  ...           12.697410        0       AAR\n",
              "3  564      BOONE    IL  ...            6.217047        1       ALU\n",
              "4  565      BROWN    IL  ...           19.200000        0       AAR\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOcIE6VPK2fU",
        "outputId": "687478d1-2050-408b-82b3-a72dd2ffedbc"
      },
      "source": [
        "print(\"Number of null values:\", mid_df[\"popother\"].isnull().sum())"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of null values: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciT-bCtDENcA",
        "outputId": "0c1bc94b-0512-4d45-a533-6ca3acb4502b"
      },
      "source": [
        "print(\"Dimensions of original data:\", mid_df.shape)\n",
        "mid_df_clean   = mid_df.dropna()\n",
        "\n",
        "print(\"Dimensions of modified data:\", mid_df_clean.shape)\n",
        "\n",
        "print(\"Number of null values:\", mid_df_clean[\"popother\"].isnull().sum())"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensions of original data: (437, 28)\n",
            "Dimensions of modified data: (437, 28)\n",
            "Number of null values: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "3BJRXqTlEfJd",
        "outputId": "9e487abf-46c3-4edb-cc61-1c6a7ce549f0"
      },
      "source": [
        "mid_df_clean.head()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PID</th>\n",
              "      <th>county</th>\n",
              "      <th>state</th>\n",
              "      <th>area</th>\n",
              "      <th>poptotal</th>\n",
              "      <th>popdensity</th>\n",
              "      <th>popwhite</th>\n",
              "      <th>popblack</th>\n",
              "      <th>popamerindian</th>\n",
              "      <th>popasian</th>\n",
              "      <th>popother</th>\n",
              "      <th>percwhite</th>\n",
              "      <th>percblack</th>\n",
              "      <th>percamerindan</th>\n",
              "      <th>percasian</th>\n",
              "      <th>percother</th>\n",
              "      <th>popadults</th>\n",
              "      <th>perchsd</th>\n",
              "      <th>percollege</th>\n",
              "      <th>percprof</th>\n",
              "      <th>poppovertyknown</th>\n",
              "      <th>percpovertyknown</th>\n",
              "      <th>percbelowpoverty</th>\n",
              "      <th>percchildbelowpovert</th>\n",
              "      <th>percadultpoverty</th>\n",
              "      <th>percelderlypoverty</th>\n",
              "      <th>inmetro</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>561</td>\n",
              "      <td>ADAMS</td>\n",
              "      <td>IL</td>\n",
              "      <td>0.052</td>\n",
              "      <td>66090</td>\n",
              "      <td>1270.961540</td>\n",
              "      <td>63917</td>\n",
              "      <td>1702</td>\n",
              "      <td>98</td>\n",
              "      <td>249</td>\n",
              "      <td>124</td>\n",
              "      <td>96.712059</td>\n",
              "      <td>2.575276</td>\n",
              "      <td>0.148283</td>\n",
              "      <td>0.376759</td>\n",
              "      <td>0.187623</td>\n",
              "      <td>43298</td>\n",
              "      <td>75.107395</td>\n",
              "      <td>19.631392</td>\n",
              "      <td>4.355859</td>\n",
              "      <td>63628</td>\n",
              "      <td>96.274777</td>\n",
              "      <td>13.151443</td>\n",
              "      <td>18.011717</td>\n",
              "      <td>11.009776</td>\n",
              "      <td>12.443812</td>\n",
              "      <td>0</td>\n",
              "      <td>AAR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>562</td>\n",
              "      <td>ALEXANDER</td>\n",
              "      <td>IL</td>\n",
              "      <td>0.014</td>\n",
              "      <td>10626</td>\n",
              "      <td>759.000000</td>\n",
              "      <td>7054</td>\n",
              "      <td>3496</td>\n",
              "      <td>19</td>\n",
              "      <td>48</td>\n",
              "      <td>9</td>\n",
              "      <td>66.384340</td>\n",
              "      <td>32.900433</td>\n",
              "      <td>0.178807</td>\n",
              "      <td>0.451722</td>\n",
              "      <td>0.084698</td>\n",
              "      <td>6724</td>\n",
              "      <td>59.726353</td>\n",
              "      <td>11.243308</td>\n",
              "      <td>2.870315</td>\n",
              "      <td>10529</td>\n",
              "      <td>99.087145</td>\n",
              "      <td>32.244278</td>\n",
              "      <td>45.826514</td>\n",
              "      <td>27.385647</td>\n",
              "      <td>25.228976</td>\n",
              "      <td>0</td>\n",
              "      <td>LHR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>563</td>\n",
              "      <td>BOND</td>\n",
              "      <td>IL</td>\n",
              "      <td>0.022</td>\n",
              "      <td>14991</td>\n",
              "      <td>681.409091</td>\n",
              "      <td>14477</td>\n",
              "      <td>429</td>\n",
              "      <td>35</td>\n",
              "      <td>16</td>\n",
              "      <td>34</td>\n",
              "      <td>96.571276</td>\n",
              "      <td>2.861717</td>\n",
              "      <td>0.233473</td>\n",
              "      <td>0.106731</td>\n",
              "      <td>0.226803</td>\n",
              "      <td>9669</td>\n",
              "      <td>69.334988</td>\n",
              "      <td>17.033819</td>\n",
              "      <td>4.488572</td>\n",
              "      <td>14235</td>\n",
              "      <td>94.956974</td>\n",
              "      <td>12.068844</td>\n",
              "      <td>14.036061</td>\n",
              "      <td>10.852090</td>\n",
              "      <td>12.697410</td>\n",
              "      <td>0</td>\n",
              "      <td>AAR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>564</td>\n",
              "      <td>BOONE</td>\n",
              "      <td>IL</td>\n",
              "      <td>0.017</td>\n",
              "      <td>30806</td>\n",
              "      <td>1812.117650</td>\n",
              "      <td>29344</td>\n",
              "      <td>127</td>\n",
              "      <td>46</td>\n",
              "      <td>150</td>\n",
              "      <td>1139</td>\n",
              "      <td>95.254171</td>\n",
              "      <td>0.412257</td>\n",
              "      <td>0.149322</td>\n",
              "      <td>0.486918</td>\n",
              "      <td>3.697332</td>\n",
              "      <td>19272</td>\n",
              "      <td>75.472188</td>\n",
              "      <td>17.278954</td>\n",
              "      <td>4.197800</td>\n",
              "      <td>30337</td>\n",
              "      <td>98.477569</td>\n",
              "      <td>7.209019</td>\n",
              "      <td>11.179536</td>\n",
              "      <td>5.536013</td>\n",
              "      <td>6.217047</td>\n",
              "      <td>1</td>\n",
              "      <td>ALU</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>565</td>\n",
              "      <td>BROWN</td>\n",
              "      <td>IL</td>\n",
              "      <td>0.018</td>\n",
              "      <td>5836</td>\n",
              "      <td>324.222222</td>\n",
              "      <td>5264</td>\n",
              "      <td>547</td>\n",
              "      <td>14</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>90.198766</td>\n",
              "      <td>9.372858</td>\n",
              "      <td>0.239890</td>\n",
              "      <td>0.085675</td>\n",
              "      <td>0.102810</td>\n",
              "      <td>3979</td>\n",
              "      <td>68.861523</td>\n",
              "      <td>14.475999</td>\n",
              "      <td>3.367680</td>\n",
              "      <td>4815</td>\n",
              "      <td>82.505140</td>\n",
              "      <td>13.520249</td>\n",
              "      <td>13.022889</td>\n",
              "      <td>11.143211</td>\n",
              "      <td>19.200000</td>\n",
              "      <td>0</td>\n",
              "      <td>AAR</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PID     county state  ...  percelderlypoverty  inmetro  category\n",
              "0  561      ADAMS    IL  ...           12.443812        0       AAR\n",
              "1  562  ALEXANDER    IL  ...           25.228976        0       LHR\n",
              "2  563       BOND    IL  ...           12.697410        0       AAR\n",
              "3  564      BOONE    IL  ...            6.217047        1       ALU\n",
              "4  565      BROWN    IL  ...           19.200000        0       AAR\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duDsnym96zG6"
      },
      "source": [
        "dummies = pd.get_dummies(mid_df_clean[['county', 'state', 'category']])\n",
        "\n",
        "y = mid_df_clean.popother\n",
        "\n",
        "# Drop the column with the independent variable (Salary), and columns for which we created dummy variables\n",
        "X_ = mid_df_clean.drop(['popother', 'county', 'state', 'category'], axis=1).astype('float64')\n",
        "\n",
        "# Define the feature set X.\n",
        "X = pd.concat([X_, dummies[['county_ADAMS', 'state_IL', 'category_AAR']]], axis=1)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wolrQe1a30oM"
      },
      "source": [
        "def forward(predictors):\n",
        "\n",
        "    remaining_predictors = [p for p in X.columns if p not in predictors]\n",
        "    \n",
        "    tic = time.time()\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    for p in remaining_predictors:\n",
        "        results.append(processSubset(predictors+[p]))\n",
        "    \n",
        "    models = pd.DataFrame(results)\n",
        "    \n",
        "    best_model = models.loc[models['RSS'].argmin()]\n",
        "    \n",
        "    toc = time.time()\n",
        "    print(\"Processed \", models.shape[0], \"models on\", len(predictors)+1, \"predictors in\", (toc-tic), \"seconds.\")\n",
        "    \n",
        "    return best_model"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "nFQbqmnc4Oks",
        "outputId": "079849bb-825e-424e-a587-723e023da6ae"
      },
      "source": [
        "models_fwd = pd.DataFrame(columns=[\"RSS\", \"model\"])\n",
        "\n",
        "tic = time.time()\n",
        "predictors = []\n",
        "\n",
        "for i in range(1,len(X.columns)+1):    \n",
        "    models_fwd.loc[i] = forward(predictors)\n",
        "    predictors = models_fwd.loc[i][\"model\"].model.exog_names\n",
        "\n",
        "toc = time.time()\n",
        "print(\"Total elapsed time:\", (toc-tic), \"seconds.\")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-aae775bf62e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmodels_fwd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mpredictors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels_fwd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-ffa83d0ae1d4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(predictors)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mremaining_predictors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessSubset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictors\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'processSubset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "id": "5O7S901lNXyz",
        "outputId": "f3830721-d6bc-4fcd-d2a5-e985e6a858e8"
      },
      "source": [
        "print(models_fwd.loc[1, \"model\"].summary())\n",
        "print(models_fwd.loc[2, \"model\"].summary())"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 1",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-7674c3fb751c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels_fwd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels_fwd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    871\u001b[0m                     \u001b[0;31m# AttributeError for IntervalTree get_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    784\u001b[0m                 \u001b[0;31m# We don't need to check for tuples here because those are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                 \u001b[0;31m#  caught by the _is_nested_tuple_indexer check above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m                 \u001b[0msection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0;31m# We should never have a scalar section here, because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;31m# fall thru to straight lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_slice_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_obj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_label\u001b[0;34m(self, label, axis)\u001b[0m\n\u001b[1;32m   1057\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0;31m# GH#5667 this will fail if the label is not present in the axis.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_lowerdim_multi_index_axis0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mxs\u001b[0;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[1;32m   3491\u001b[0m             \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3493\u001b[0;31m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3495\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2900\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 1"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWv8Ifu3fAp4"
      },
      "source": [
        "Backward Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Epca3uMVfDL0"
      },
      "source": [
        "def backward(predictors):\n",
        "    \n",
        "    tic = time.time()\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    for combo in itertools.combinations(predictors, len(predictors)-1):\n",
        "        print(combo)\n",
        "        results.append(processSubset(combo))\n",
        "    \n",
        "    models = pd.DataFrame(results)\n",
        "  \n",
        "    best_model = models.loc[models['RSS'].argmin()]\n",
        "    \n",
        "    toc = time.time()\n",
        "    print(\"Processed \", models.shape[0], \"models on\", len(predictors)-1, \"predictors in\", (toc-tic), \"seconds.\")\n",
        "  \n",
        "    return best_model"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "gwYNpYSJhDH4",
        "outputId": "2444d88d-73e6-43f6-840f-bf4d7528abad"
      },
      "source": [
        "models_bwd = pd.DataFrame(columns=[\"RSS\", \"model\"], index = range(1,len(X.columns)))\n",
        "\n",
        "tic = time.time()\n",
        "predictors = X.columns\n",
        "\n",
        "while(len(predictors) > 1):  \n",
        "    models_bwd.loc[len(predictors)-1] = backward(predictors)\n",
        "    predictors = models_bwd.loc[len(predictors)-1][\"model\"].model.exog_names\n",
        "\n",
        "toc = time.time()\n",
        "print(\"Total elapsed time:\", (toc-tic), \"seconds.\")"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('PID', 'area', 'poptotal', 'popdensity', 'popwhite', 'popblack', 'popamerindian', 'popasian', 'percwhite', 'percblack', 'percamerindan', 'percasian', 'percother', 'popadults', 'perchsd', 'percollege', 'percprof', 'poppovertyknown', 'percpovertyknown', 'percbelowpoverty', 'percchildbelowpovert', 'percadultpoverty', 'percelderlypoverty', 'inmetro', 'county_ADAMS', 'state_IL')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-34805a2673ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmodels_bwd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mpredictors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels_bwd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-d503e23c95e4>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(predictors)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcombo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombinations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessSubset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'processSubset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "wBMwm6zshGdi",
        "outputId": "ec8fcf5b-1540-47ba-bd7c-687dd716488a"
      },
      "source": [
        "print(\"-----------------\")\n",
        "print(\"Foward Selection:\")\n",
        "print(\"-----------------\")\n",
        "print(models_fwd.loc[7, \"model\"].params)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------\n",
            "Foward Selection:\n",
            "-----------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 7",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-39f451c2bdd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Foward Selection:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-----------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels_fwd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    871\u001b[0m                     \u001b[0;31m# AttributeError for IntervalTree get_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    784\u001b[0m                 \u001b[0;31m# We don't need to check for tuples here because those are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                 \u001b[0;31m#  caught by the _is_nested_tuple_indexer check above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m                 \u001b[0msection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0;31m# We should never have a scalar section here, because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;31m# fall thru to straight lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_slice_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_obj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_label\u001b[0;34m(self, label, axis)\u001b[0m\n\u001b[1;32m   1057\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0;31m# GH#5667 this will fail if the label is not present in the axis.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_lowerdim_multi_index_axis0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mxs\u001b[0;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[1;32m   3491\u001b[0m             \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3493\u001b[0;31m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3495\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2900\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 7"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "Y2MkrZ7UhPr6",
        "outputId": "16d5f332-5249-45e3-e4e1-a2475b9835e8"
      },
      "source": [
        "print(\"-------------------\")\n",
        "print(\"Backward Selection:\")\n",
        "print(\"-------------------\")\n",
        "print(models_bwd.loc[7, \"model\"].params)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------\n",
            "Backward Selection:\n",
            "-------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-5b42b4434dee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Backward Selection:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels_bwd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'params'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boV8GcT0ingI"
      },
      "source": [
        "Ridge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJQPFM1IinRt"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import scale \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SvV07uzi4W2",
        "outputId": "17c5f090-c533-4050-a280-9927fdd8fe53"
      },
      "source": [
        "alphas = 10**np.linspace(10,-2,100) *0.5\n",
        "alphas"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5.00000000e+09, 3.78231664e+09, 2.86118383e+09, 2.16438064e+09,\n",
              "       1.63727458e+09, 1.23853818e+09, 9.36908711e+08, 7.08737081e+08,\n",
              "       5.36133611e+08, 4.05565415e+08, 3.06795364e+08, 2.32079442e+08,\n",
              "       1.75559587e+08, 1.32804389e+08, 1.00461650e+08, 7.59955541e+07,\n",
              "       5.74878498e+07, 4.34874501e+07, 3.28966612e+07, 2.48851178e+07,\n",
              "       1.88246790e+07, 1.42401793e+07, 1.07721735e+07, 8.14875417e+06,\n",
              "       6.16423370e+06, 4.66301673e+06, 3.52740116e+06, 2.66834962e+06,\n",
              "       2.01850863e+06, 1.52692775e+06, 1.15506485e+06, 8.73764200e+05,\n",
              "       6.60970574e+05, 5.00000000e+05, 3.78231664e+05, 2.86118383e+05,\n",
              "       2.16438064e+05, 1.63727458e+05, 1.23853818e+05, 9.36908711e+04,\n",
              "       7.08737081e+04, 5.36133611e+04, 4.05565415e+04, 3.06795364e+04,\n",
              "       2.32079442e+04, 1.75559587e+04, 1.32804389e+04, 1.00461650e+04,\n",
              "       7.59955541e+03, 5.74878498e+03, 4.34874501e+03, 3.28966612e+03,\n",
              "       2.48851178e+03, 1.88246790e+03, 1.42401793e+03, 1.07721735e+03,\n",
              "       8.14875417e+02, 6.16423370e+02, 4.66301673e+02, 3.52740116e+02,\n",
              "       2.66834962e+02, 2.01850863e+02, 1.52692775e+02, 1.15506485e+02,\n",
              "       8.73764200e+01, 6.60970574e+01, 5.00000000e+01, 3.78231664e+01,\n",
              "       2.86118383e+01, 2.16438064e+01, 1.63727458e+01, 1.23853818e+01,\n",
              "       9.36908711e+00, 7.08737081e+00, 5.36133611e+00, 4.05565415e+00,\n",
              "       3.06795364e+00, 2.32079442e+00, 1.75559587e+00, 1.32804389e+00,\n",
              "       1.00461650e+00, 7.59955541e-01, 5.74878498e-01, 4.34874501e-01,\n",
              "       3.28966612e-01, 2.48851178e-01, 1.88246790e-01, 1.42401793e-01,\n",
              "       1.07721735e-01, 8.14875417e-02, 6.16423370e-02, 4.66301673e-02,\n",
              "       3.52740116e-02, 2.66834962e-02, 2.01850863e-02, 1.52692775e-02,\n",
              "       1.15506485e-02, 8.73764200e-03, 6.60970574e-03, 5.00000000e-03])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JH0EAS0Ji_7P",
        "outputId": "707b34c6-c134-484b-9725-a906c6f3b2e5"
      },
      "source": [
        "ridge = Ridge(normalize = True)\n",
        "coefs = []\n",
        "\n",
        "for a in alphas:\n",
        "    ridge.set_params(alpha = a)\n",
        "    ridge.fit(X, y)\n",
        "    coefs.append(ridge.coef_)\n",
        "    \n",
        "np.shape(coefs)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 27)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "URLV9i3TjLGn",
        "outputId": "786f454b-b79b-430e-be34-9ab2e204dc80"
      },
      "source": [
        "ax = plt.gca()\n",
        "ax.plot(alphas, coefs)\n",
        "ax.set_xscale('log')\n",
        "plt.axis('tight')\n",
        "plt.xlabel('alpha')\n",
        "plt.ylabel('weights')"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'weights')"
            ]
          },
          "metadata": {},
          "execution_count": 53
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8IH-D67jV5f"
      },
      "source": [
        "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-z_CakajeF-",
        "outputId": "e8a62964-f791-4ff3-de8b-d48498bb01ca"
      },
      "source": [
        "ridge2 = Ridge(alpha = 4, normalize = True)\n",
        "ridge2.fit(X_train, y_train)            \n",
        "pred2 = ridge2.predict(X_test)           \n",
        "print(pd.Series(ridge2.coef_, index = X.columns)) \n",
        "print(mean_squared_error(y_test, pred2))          "
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PID                        -0.299662\n",
            "area                    19672.891432\n",
            "poptotal                    0.005559\n",
            "popdensity                  0.107905\n",
            "popwhite                    0.007308\n",
            "popblack                    0.025711\n",
            "popamerindian               1.423946\n",
            "popasian                    0.190106\n",
            "percwhite                 -27.905243\n",
            "percblack                  39.895035\n",
            "percamerindan             -15.167900\n",
            "percasian                 492.688724\n",
            "percother                1246.179354\n",
            "popadults                   0.008617\n",
            "perchsd                   -44.705229\n",
            "percollege                -27.132213\n",
            "percprof                  -41.799190\n",
            "poppovertyknown             0.005668\n",
            "percpovertyknown           37.405342\n",
            "percbelowpoverty            5.621531\n",
            "percchildbelowpovert        7.266809\n",
            "percadultpoverty           -1.475295\n",
            "percelderlypoverty         47.150888\n",
            "inmetro                  -339.571021\n",
            "county_ADAMS                0.000000\n",
            "state_IL                  779.399278\n",
            "category_AAR              212.194630\n",
            "dtype: float64\n",
            "37551325.82717369\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUlYZ_m9joAI",
        "outputId": "0df78a2b-a705-46d8-b958-5d7f7f82336e"
      },
      "source": [
        "ridge3 = Ridge(alpha = 10**10, normalize =True)\n",
        "ridge3.fit(X_train, y_train)            \n",
        "pred3 = ridge3.predict(X_test)           \n",
        "print(pd.Series(ridge3.coef_, index = X.columns)) \n",
        "print(mean_squared_error(y_test, pred3))          "
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PID                    -1.897446e-10\n",
            "area                    1.754132e-05\n",
            "poptotal                6.405853e-12\n",
            "popdensity              1.960320e-10\n",
            "popwhite                9.142584e-12\n",
            "popblack                2.680117e-11\n",
            "popamerindian           1.959886e-09\n",
            "popasian                1.922816e-10\n",
            "percwhite              -9.460102e-08\n",
            "percblack               1.728751e-07\n",
            "percamerindan          -5.019685e-09\n",
            "percasian               1.347758e-06\n",
            "percother               1.464978e-06\n",
            "popadults               9.928581e-12\n",
            "perchsd                 7.299133e-09\n",
            "percollege              5.734718e-08\n",
            "percprof                1.448612e-07\n",
            "poppovertyknown         6.521199e-12\n",
            "percpovertyknown        2.978614e-08\n",
            "percbelowpoverty        4.308746e-09\n",
            "percchildbelowpovert    1.475459e-08\n",
            "percadultpoverty       -5.460545e-10\n",
            "percelderlypoverty     -2.433599e-08\n",
            "inmetro                 7.030544e-07\n",
            "county_ADAMS            0.000000e+00\n",
            "state_IL                7.634365e-07\n",
            "category_AAR           -4.377010e-07\n",
            "dtype: float64\n",
            "8882775.862121405\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlvzP2WTjwvU",
        "outputId": "d875f22b-12ed-4567-f143-f52c12fa26bf"
      },
      "source": [
        "ridge2 = Ridge(alpha = 0, normalize = True)\n",
        "ridge2.fit(X_train, y_train)             \n",
        "pred = ridge2.predict(X_test)           \n",
        "print(pd.Series(ridge2.coef_, index = X.columns)) \n",
        "print(mean_squared_error(y_test, pred))           "
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PID                     8.550784e-14\n",
            "area                   -4.982098e-09\n",
            "poptotal                1.000000e+00\n",
            "popdensity             -1.162652e-14\n",
            "popwhite               -1.000000e+00\n",
            "popblack               -1.000000e+00\n",
            "popamerindian          -1.000000e+00\n",
            "popasian               -1.000000e+00\n",
            "percwhite              -3.451720e-04\n",
            "percblack              -3.451720e-04\n",
            "percamerindan          -3.451720e-04\n",
            "percasian              -3.451720e-04\n",
            "percother              -3.451720e-04\n",
            "popadults              -7.806507e-15\n",
            "perchsd                 1.684963e-12\n",
            "percollege             -8.180625e-12\n",
            "percprof               -9.799560e-13\n",
            "poppovertyknown        -1.249485e-14\n",
            "percpovertyknown        4.257125e-12\n",
            "percbelowpoverty       -5.593969e-11\n",
            "percchildbelowpovert    1.810025e-11\n",
            "percadultpoverty        2.878506e-11\n",
            "percelderlypoverty      1.003640e-11\n",
            "inmetro                 1.997128e-11\n",
            "county_ADAMS           -6.427124e-10\n",
            "state_IL                1.237168e-10\n",
            "category_AAR            5.692447e-11\n",
            "dtype: float64\n",
            "1.9397901133907583e-20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmhhrOTOj5oA",
        "outputId": "7afa32eb-91a8-4681-dd4d-e8fa8964f1da"
      },
      "source": [
        "ridgecv = RidgeCV(alphas = alphas, scoring = 'neg_mean_squared_error', normalize = True)\n",
        "ridgecv.fit(X_train, y_train)\n",
        "ridgecv.alpha_"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), _RidgeGCV())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.005"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCPzjMvQj-bM",
        "outputId": "be8dd6e1-f63b-40e7-aa83-588fb11d9056"
      },
      "source": [
        "alphas"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5.00000000e+09, 3.78231664e+09, 2.86118383e+09, 2.16438064e+09,\n",
              "       1.63727458e+09, 1.23853818e+09, 9.36908711e+08, 7.08737081e+08,\n",
              "       5.36133611e+08, 4.05565415e+08, 3.06795364e+08, 2.32079442e+08,\n",
              "       1.75559587e+08, 1.32804389e+08, 1.00461650e+08, 7.59955541e+07,\n",
              "       5.74878498e+07, 4.34874501e+07, 3.28966612e+07, 2.48851178e+07,\n",
              "       1.88246790e+07, 1.42401793e+07, 1.07721735e+07, 8.14875417e+06,\n",
              "       6.16423370e+06, 4.66301673e+06, 3.52740116e+06, 2.66834962e+06,\n",
              "       2.01850863e+06, 1.52692775e+06, 1.15506485e+06, 8.73764200e+05,\n",
              "       6.60970574e+05, 5.00000000e+05, 3.78231664e+05, 2.86118383e+05,\n",
              "       2.16438064e+05, 1.63727458e+05, 1.23853818e+05, 9.36908711e+04,\n",
              "       7.08737081e+04, 5.36133611e+04, 4.05565415e+04, 3.06795364e+04,\n",
              "       2.32079442e+04, 1.75559587e+04, 1.32804389e+04, 1.00461650e+04,\n",
              "       7.59955541e+03, 5.74878498e+03, 4.34874501e+03, 3.28966612e+03,\n",
              "       2.48851178e+03, 1.88246790e+03, 1.42401793e+03, 1.07721735e+03,\n",
              "       8.14875417e+02, 6.16423370e+02, 4.66301673e+02, 3.52740116e+02,\n",
              "       2.66834962e+02, 2.01850863e+02, 1.52692775e+02, 1.15506485e+02,\n",
              "       8.73764200e+01, 6.60970574e+01, 5.00000000e+01, 3.78231664e+01,\n",
              "       2.86118383e+01, 2.16438064e+01, 1.63727458e+01, 1.23853818e+01,\n",
              "       9.36908711e+00, 7.08737081e+00, 5.36133611e+00, 4.05565415e+00,\n",
              "       3.06795364e+00, 2.32079442e+00, 1.75559587e+00, 1.32804389e+00,\n",
              "       1.00461650e+00, 7.59955541e-01, 5.74878498e-01, 4.34874501e-01,\n",
              "       3.28966612e-01, 2.48851178e-01, 1.88246790e-01, 1.42401793e-01,\n",
              "       1.07721735e-01, 8.14875417e-02, 6.16423370e-02, 4.66301673e-02,\n",
              "       3.52740116e-02, 2.66834962e-02, 2.01850863e-02, 1.52692775e-02,\n",
              "       1.15506485e-02, 8.73764200e-03, 6.60970574e-03, 5.00000000e-03])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3r5noopkBXk",
        "outputId": "5fa5c6d1-8ad7-4665-89ec-1e698f164d9c"
      },
      "source": [
        "ridge4 = Ridge(alpha = ridgecv.alpha_, normalize = True)\n",
        "ridge4.fit(X_train, y_train)\n",
        "mean_squared_error(y_test, ridge4.predict(X_test))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34990531.67588542"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQbztKq6kGKC",
        "outputId": "f26a0461-f7de-421f-e4fb-1ab500d07a44"
      },
      "source": [
        "ridge4.fit(X, y)\n",
        "pd.Series(ridge4.coef_, index = X.columns)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PID                         0.357908\n",
              "area                   -13184.980159\n",
              "poptotal                    0.009525\n",
              "popdensity                 -0.408463\n",
              "popwhite                   -0.038084\n",
              "popblack                    0.011000\n",
              "popamerindian               0.041073\n",
              "popasian                    1.972275\n",
              "percwhite                  13.486209\n",
              "percblack                  24.987629\n",
              "percamerindan             -54.647386\n",
              "percasian               -4605.989018\n",
              "percother                2283.345629\n",
              "popadults                   0.016170\n",
              "perchsd                   -21.277870\n",
              "percollege                 10.480325\n",
              "percprof                   35.281861\n",
              "poppovertyknown             0.007772\n",
              "percpovertyknown         -145.813403\n",
              "percbelowpoverty          123.043817\n",
              "percchildbelowpovert      -89.346260\n",
              "percadultpoverty          136.461653\n",
              "percelderlypoverty       -140.078088\n",
              "inmetro                  1611.816306\n",
              "county_ADAMS            -1254.468307\n",
              "state_IL                 -106.204414\n",
              "category_AAR              364.553987\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEEskKbPkOmm"
      },
      "source": [
        "Lasso"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AdCpaNIWkSOa",
        "outputId": "ee00cc1e-610c-40d6-b272-11577af3fe9e"
      },
      "source": [
        "lasso = Lasso(max_iter = 10000, normalize = True)\n",
        "coefs = []\n",
        "\n",
        "for a in alphas:\n",
        "    lasso.set_params(alpha=a)\n",
        "    lasso.fit(scale(X_train), y_train)\n",
        "    coefs.append(lasso.coef_)\n",
        "    \n",
        "ax = plt.gca()\n",
        "ax.plot(alphas*2, coefs)\n",
        "ax.set_xscale('log')\n",
        "plt.axis('tight')\n",
        "plt.xlabel('alpha')\n",
        "plt.ylabel('weights')"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.056e+07, tolerance: 1.481e+07\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.885e+07, tolerance: 1.481e+07\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.305e+08, tolerance: 1.481e+07\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.110e+08, tolerance: 1.481e+07\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.638e+08, tolerance: 1.481e+07\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.952e+08, tolerance: 1.481e+07\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.129e+08, tolerance: 1.481e+07\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.222e+08, tolerance: 1.481e+07\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.268e+08, tolerance: 1.481e+07\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.288e+08, tolerance: 1.481e+07\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.295e+08, tolerance: 1.481e+07\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.295e+08, tolerance: 1.481e+07\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.299e+08, tolerance: 1.481e+07\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.298e+08, tolerance: 1.481e+07\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.294e+08, tolerance: 1.481e+07\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.290e+08, tolerance: 1.481e+07\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.285e+08, tolerance: 1.481e+07\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.282e+08, tolerance: 1.481e+07\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.278e+08, tolerance: 1.481e+07\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'weights')"
            ]
          },
          "metadata": {},
          "execution_count": 62
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gf3a3fwka30",
        "outputId": "13012723-26ee-49ed-d8c1-b9139871f8be"
      },
      "source": [
        "lassocv = LassoCV(alphas = None, cv = 10, max_iter = 100000, normalize = True)\n",
        "lassocv.fit(X_train, y_train)\n",
        "\n",
        "lasso.set_params(alpha=lassocv.alpha_)\n",
        "lasso.fit(X_train, y_train)\n",
        "mean_squared_error(y_test, lasso.predict(X_test))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31755767.732107677"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDMIsHRLkf1d",
        "outputId": "6db6f128-b185-4d1b-eebe-a19528686de6"
      },
      "source": [
        "pd.Series(lasso.coef_, index=X.columns)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PID                         0.128082\n",
              "area                   -36024.149580\n",
              "poptotal                    0.000000\n",
              "popdensity                 -0.694901\n",
              "popwhite                   -0.001976\n",
              "popblack                    0.111844\n",
              "popamerindian               2.265916\n",
              "popasian                    1.453510\n",
              "percwhite                 115.647040\n",
              "percblack                  -0.000000\n",
              "percamerindan              -0.000000\n",
              "percasian               -2813.278271\n",
              "percother                2084.811116\n",
              "popadults                   0.000000\n",
              "perchsd                    -0.000000\n",
              "percollege                 -0.000000\n",
              "percprof                   -0.000000\n",
              "poppovertyknown             0.000000\n",
              "percpovertyknown          -85.476490\n",
              "percbelowpoverty            0.000000\n",
              "percchildbelowpovert        0.000000\n",
              "percadultpoverty           10.091136\n",
              "percelderlypoverty         -0.000000\n",
              "inmetro                   598.089455\n",
              "county_ADAMS                0.000000\n",
              "state_IL                   65.532639\n",
              "category_AAR               -0.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjBiciB3kwsb"
      },
      "source": [
        "Elastic Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7yB-d8Gkzlk",
        "outputId": "25477820-075f-4ffa-86be-6a5d2d97582c"
      },
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import absolute\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/selva86/datasets/master/midwest.csv'\n",
        "dataframe = read_csv(url, header=None)\n",
        "data = dataframe.values\n",
        "X, y = data[:, :-1], data[:, -1]\n",
        "\n",
        "model = ElasticNet(alpha=1.0, l1_ratio=0.5)\n",
        "\n",
        "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\n",
        "scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
        "\n",
        "scores = absolute(scores)\n",
        "print('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean MAE: nan (nan)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "30 fits failed out of a total of 30.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "27 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 681, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py\", line 943, in fit\n",
            "    y_numeric=True,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/base.py\", line 576, in _validate_data\n",
            "    X, y = check_X_y(X, y, **check_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 968, in check_X_y\n",
            "    estimator=estimator,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 738, in check_array\n",
            "    array = np.asarray(array, order=order, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py\", line 83, in asarray\n",
            "    return array(a, dtype, copy=False, order=order)\n",
            "ValueError: could not convert string to float: 'inmetro'\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "3 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 681, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py\", line 943, in fit\n",
            "    y_numeric=True,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/base.py\", line 576, in _validate_data\n",
            "    X, y = check_X_y(X, y, **check_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 968, in check_X_y\n",
            "    estimator=estimator,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 738, in check_array\n",
            "    array = np.asarray(array, order=order, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py\", line 83, in asarray\n",
            "    return array(a, dtype, copy=False, order=order)\n",
            "ValueError: could not convert string to float: 'WI'\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-10_lklui-P"
      },
      "source": [
        "PCR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cdv9eGW5uiQQ"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import scale \n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.cross_decomposition import PLSRegression, PLSSVD\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfizjmhTuwqb",
        "outputId": "c8c92df0-f525-4b4c-fa44-8198acbf89f9"
      },
      "source": [
        "url='https://raw.githubusercontent.com/selva86/datasets/master/midwest.csv'\n",
        "df = pd.read_csv(url).dropna()\n",
        "df.info()\n",
        "dummies = pd.get_dummies(df[['county', 'state', 'category']])"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 437 entries, 0 to 436\n",
            "Data columns (total 28 columns):\n",
            " #   Column                Non-Null Count  Dtype  \n",
            "---  ------                --------------  -----  \n",
            " 0   PID                   437 non-null    int64  \n",
            " 1   county                437 non-null    object \n",
            " 2   state                 437 non-null    object \n",
            " 3   area                  437 non-null    float64\n",
            " 4   poptotal              437 non-null    int64  \n",
            " 5   popdensity            437 non-null    float64\n",
            " 6   popwhite              437 non-null    int64  \n",
            " 7   popblack              437 non-null    int64  \n",
            " 8   popamerindian         437 non-null    int64  \n",
            " 9   popasian              437 non-null    int64  \n",
            " 10  popother              437 non-null    int64  \n",
            " 11  percwhite             437 non-null    float64\n",
            " 12  percblack             437 non-null    float64\n",
            " 13  percamerindan         437 non-null    float64\n",
            " 14  percasian             437 non-null    float64\n",
            " 15  percother             437 non-null    float64\n",
            " 16  popadults             437 non-null    int64  \n",
            " 17  perchsd               437 non-null    float64\n",
            " 18  percollege            437 non-null    float64\n",
            " 19  percprof              437 non-null    float64\n",
            " 20  poppovertyknown       437 non-null    int64  \n",
            " 21  percpovertyknown      437 non-null    float64\n",
            " 22  percbelowpoverty      437 non-null    float64\n",
            " 23  percchildbelowpovert  437 non-null    float64\n",
            " 24  percadultpoverty      437 non-null    float64\n",
            " 25  percelderlypoverty    437 non-null    float64\n",
            " 26  inmetro               437 non-null    int64  \n",
            " 27  category              437 non-null    object \n",
            "dtypes: float64(15), int64(10), object(3)\n",
            "memory usage: 99.0+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba6V6R1kvCN0"
      },
      "source": [
        "y = df.popother\n",
        "\n",
        "# Drop the column with the independent variable (Salary), and columns for which we created dummy variables\n",
        "X_ = df.drop(['popother', 'county', 'state', 'category'], axis=1).astype('float64')\n",
        "\n",
        "# Define the feature set X.\n",
        "X = pd.concat([X_, dummies[['county_ADAMS', 'state_IL', 'category_AAR']]], axis=1)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "r1voz6-1vtPs",
        "outputId": "bfc28ed9-50a8-48af-a100-571641d46fcf"
      },
      "source": [
        "pca = PCA()\n",
        "X_reduced = pca.fit_transform(scale(X)) \n",
        "pd.DataFrame(pca.components_.T) "
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.000956</td>\n",
              "      <td>-0.009323</td>\n",
              "      <td>-0.014329</td>\n",
              "      <td>0.605191</td>\n",
              "      <td>-0.079737</td>\n",
              "      <td>-0.240022</td>\n",
              "      <td>-0.019305</td>\n",
              "      <td>-0.168010</td>\n",
              "      <td>-0.130751</td>\n",
              "      <td>0.120089</td>\n",
              "      <td>-0.063613</td>\n",
              "      <td>-0.644918</td>\n",
              "      <td>0.101628</td>\n",
              "      <td>-0.147605</td>\n",
              "      <td>-0.067585</td>\n",
              "      <td>0.027944</td>\n",
              "      <td>0.043643</td>\n",
              "      <td>-0.118336</td>\n",
              "      <td>0.033520</td>\n",
              "      <td>0.093123</td>\n",
              "      <td>0.149128</td>\n",
              "      <td>-0.062335</td>\n",
              "      <td>-0.010072</td>\n",
              "      <td>0.001937</td>\n",
              "      <td>-0.000278</td>\n",
              "      <td>0.000866</td>\n",
              "      <td>-1.864768e-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.023684</td>\n",
              "      <td>0.016026</td>\n",
              "      <td>-0.076667</td>\n",
              "      <td>0.388179</td>\n",
              "      <td>0.375549</td>\n",
              "      <td>0.017731</td>\n",
              "      <td>0.186765</td>\n",
              "      <td>-0.599874</td>\n",
              "      <td>0.229266</td>\n",
              "      <td>-0.234228</td>\n",
              "      <td>0.154896</td>\n",
              "      <td>0.344180</td>\n",
              "      <td>0.089801</td>\n",
              "      <td>0.103071</td>\n",
              "      <td>0.009334</td>\n",
              "      <td>-0.100230</td>\n",
              "      <td>0.110529</td>\n",
              "      <td>0.083312</td>\n",
              "      <td>0.057705</td>\n",
              "      <td>-0.014381</td>\n",
              "      <td>0.053934</td>\n",
              "      <td>0.046751</td>\n",
              "      <td>0.007150</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>-0.000731</td>\n",
              "      <td>0.000496</td>\n",
              "      <td>-8.554078e-11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.316230</td>\n",
              "      <td>0.063195</td>\n",
              "      <td>0.142777</td>\n",
              "      <td>0.000632</td>\n",
              "      <td>0.055973</td>\n",
              "      <td>-0.059354</td>\n",
              "      <td>-0.018055</td>\n",
              "      <td>0.038419</td>\n",
              "      <td>0.043741</td>\n",
              "      <td>-0.013552</td>\n",
              "      <td>0.041207</td>\n",
              "      <td>-0.009107</td>\n",
              "      <td>-0.026424</td>\n",
              "      <td>-0.006457</td>\n",
              "      <td>-0.094870</td>\n",
              "      <td>-0.039658</td>\n",
              "      <td>0.047949</td>\n",
              "      <td>0.025856</td>\n",
              "      <td>0.000389</td>\n",
              "      <td>0.079528</td>\n",
              "      <td>-0.116482</td>\n",
              "      <td>-0.174996</td>\n",
              "      <td>0.026294</td>\n",
              "      <td>0.051728</td>\n",
              "      <td>-0.580915</td>\n",
              "      <td>-0.676503</td>\n",
              "      <td>-2.643839e-09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.302322</td>\n",
              "      <td>0.038265</td>\n",
              "      <td>0.084283</td>\n",
              "      <td>-0.009740</td>\n",
              "      <td>-0.077429</td>\n",
              "      <td>-0.045670</td>\n",
              "      <td>-0.057987</td>\n",
              "      <td>0.060384</td>\n",
              "      <td>-0.128227</td>\n",
              "      <td>-0.105381</td>\n",
              "      <td>-0.082649</td>\n",
              "      <td>-0.095344</td>\n",
              "      <td>-0.027681</td>\n",
              "      <td>0.030374</td>\n",
              "      <td>0.386673</td>\n",
              "      <td>0.054413</td>\n",
              "      <td>0.395198</td>\n",
              "      <td>0.532518</td>\n",
              "      <td>0.178827</td>\n",
              "      <td>-0.142410</td>\n",
              "      <td>0.246238</td>\n",
              "      <td>0.366351</td>\n",
              "      <td>0.015857</td>\n",
              "      <td>-0.001734</td>\n",
              "      <td>-0.009645</td>\n",
              "      <td>0.002023</td>\n",
              "      <td>-2.944233e-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.319960</td>\n",
              "      <td>0.036528</td>\n",
              "      <td>0.113152</td>\n",
              "      <td>0.005853</td>\n",
              "      <td>0.029840</td>\n",
              "      <td>-0.063156</td>\n",
              "      <td>-0.017895</td>\n",
              "      <td>0.027515</td>\n",
              "      <td>0.045488</td>\n",
              "      <td>-0.032325</td>\n",
              "      <td>0.010731</td>\n",
              "      <td>-0.023733</td>\n",
              "      <td>0.004114</td>\n",
              "      <td>0.052083</td>\n",
              "      <td>-0.019625</td>\n",
              "      <td>-0.028271</td>\n",
              "      <td>0.139376</td>\n",
              "      <td>0.203427</td>\n",
              "      <td>-0.201652</td>\n",
              "      <td>0.281841</td>\n",
              "      <td>-0.216389</td>\n",
              "      <td>-0.357404</td>\n",
              "      <td>0.024114</td>\n",
              "      <td>0.531803</td>\n",
              "      <td>0.480526</td>\n",
              "      <td>0.004579</td>\n",
              "      <td>1.143248e-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.287480</td>\n",
              "      <td>0.117286</td>\n",
              "      <td>0.183743</td>\n",
              "      <td>-0.007410</td>\n",
              "      <td>0.074061</td>\n",
              "      <td>-0.043939</td>\n",
              "      <td>-0.030461</td>\n",
              "      <td>0.042349</td>\n",
              "      <td>-0.028063</td>\n",
              "      <td>-0.042716</td>\n",
              "      <td>0.083950</td>\n",
              "      <td>0.032617</td>\n",
              "      <td>-0.109792</td>\n",
              "      <td>-0.151630</td>\n",
              "      <td>-0.145987</td>\n",
              "      <td>-0.065511</td>\n",
              "      <td>-0.089028</td>\n",
              "      <td>-0.343017</td>\n",
              "      <td>0.649046</td>\n",
              "      <td>-0.395395</td>\n",
              "      <td>-0.012201</td>\n",
              "      <td>0.018336</td>\n",
              "      <td>0.006318</td>\n",
              "      <td>0.202300</td>\n",
              "      <td>0.208895</td>\n",
              "      <td>-0.000760</td>\n",
              "      <td>5.484149e-09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.275741</td>\n",
              "      <td>0.104237</td>\n",
              "      <td>0.032984</td>\n",
              "      <td>0.182212</td>\n",
              "      <td>-0.007972</td>\n",
              "      <td>0.103474</td>\n",
              "      <td>0.005567</td>\n",
              "      <td>0.056146</td>\n",
              "      <td>0.010587</td>\n",
              "      <td>-0.111857</td>\n",
              "      <td>0.109641</td>\n",
              "      <td>0.101278</td>\n",
              "      <td>-0.140526</td>\n",
              "      <td>-0.136298</td>\n",
              "      <td>0.645094</td>\n",
              "      <td>0.286755</td>\n",
              "      <td>-0.313466</td>\n",
              "      <td>-0.359403</td>\n",
              "      <td>-0.219121</td>\n",
              "      <td>0.105920</td>\n",
              "      <td>0.078935</td>\n",
              "      <td>0.017157</td>\n",
              "      <td>0.002615</td>\n",
              "      <td>-0.005222</td>\n",
              "      <td>0.005740</td>\n",
              "      <td>-0.000064</td>\n",
              "      <td>4.878355e-11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.280154</td>\n",
              "      <td>0.064794</td>\n",
              "      <td>0.163190</td>\n",
              "      <td>-0.016490</td>\n",
              "      <td>0.175595</td>\n",
              "      <td>-0.079622</td>\n",
              "      <td>-0.000917</td>\n",
              "      <td>0.123258</td>\n",
              "      <td>0.225179</td>\n",
              "      <td>0.143063</td>\n",
              "      <td>0.082667</td>\n",
              "      <td>-0.047235</td>\n",
              "      <td>0.074266</td>\n",
              "      <td>0.064202</td>\n",
              "      <td>-0.377954</td>\n",
              "      <td>-0.041421</td>\n",
              "      <td>-0.198584</td>\n",
              "      <td>-0.054525</td>\n",
              "      <td>-0.409156</td>\n",
              "      <td>-0.043975</td>\n",
              "      <td>0.311865</td>\n",
              "      <td>0.530351</td>\n",
              "      <td>0.020446</td>\n",
              "      <td>0.088325</td>\n",
              "      <td>0.071376</td>\n",
              "      <td>0.003063</td>\n",
              "      <td>6.289658e-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-0.193227</td>\n",
              "      <td>-0.191122</td>\n",
              "      <td>0.217187</td>\n",
              "      <td>-0.080191</td>\n",
              "      <td>0.193021</td>\n",
              "      <td>-0.420085</td>\n",
              "      <td>-0.069996</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>0.227000</td>\n",
              "      <td>0.067305</td>\n",
              "      <td>0.007042</td>\n",
              "      <td>-0.017394</td>\n",
              "      <td>-0.235251</td>\n",
              "      <td>-0.017505</td>\n",
              "      <td>0.141113</td>\n",
              "      <td>0.075701</td>\n",
              "      <td>0.048060</td>\n",
              "      <td>-0.005394</td>\n",
              "      <td>-0.000577</td>\n",
              "      <td>-0.030988</td>\n",
              "      <td>0.007796</td>\n",
              "      <td>-0.038385</td>\n",
              "      <td>0.015444</td>\n",
              "      <td>-0.000842</td>\n",
              "      <td>0.000504</td>\n",
              "      <td>-0.000091</td>\n",
              "      <td>-7.149193e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.211613</td>\n",
              "      <td>0.128547</td>\n",
              "      <td>-0.119615</td>\n",
              "      <td>-0.150842</td>\n",
              "      <td>-0.165095</td>\n",
              "      <td>0.089988</td>\n",
              "      <td>-0.039497</td>\n",
              "      <td>-0.302822</td>\n",
              "      <td>-0.547057</td>\n",
              "      <td>-0.250372</td>\n",
              "      <td>-0.137507</td>\n",
              "      <td>0.047484</td>\n",
              "      <td>0.148994</td>\n",
              "      <td>-0.018236</td>\n",
              "      <td>-0.146107</td>\n",
              "      <td>-0.138882</td>\n",
              "      <td>-0.156809</td>\n",
              "      <td>-0.067097</td>\n",
              "      <td>-0.087855</td>\n",
              "      <td>0.106057</td>\n",
              "      <td>0.017387</td>\n",
              "      <td>0.099868</td>\n",
              "      <td>0.003733</td>\n",
              "      <td>-0.000277</td>\n",
              "      <td>0.001237</td>\n",
              "      <td>0.000430</td>\n",
              "      <td>-5.180564e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.001045</td>\n",
              "      <td>0.176596</td>\n",
              "      <td>-0.166221</td>\n",
              "      <td>0.315965</td>\n",
              "      <td>-0.136578</td>\n",
              "      <td>0.532365</td>\n",
              "      <td>0.109508</td>\n",
              "      <td>0.380721</td>\n",
              "      <td>0.262171</td>\n",
              "      <td>0.013587</td>\n",
              "      <td>0.197817</td>\n",
              "      <td>-0.029049</td>\n",
              "      <td>0.146137</td>\n",
              "      <td>-0.007732</td>\n",
              "      <td>-0.092121</td>\n",
              "      <td>0.008400</td>\n",
              "      <td>0.138802</td>\n",
              "      <td>0.072975</td>\n",
              "      <td>0.070870</td>\n",
              "      <td>-0.047107</td>\n",
              "      <td>-0.018868</td>\n",
              "      <td>-0.035125</td>\n",
              "      <td>-0.026160</td>\n",
              "      <td>0.003496</td>\n",
              "      <td>-0.002493</td>\n",
              "      <td>-0.000342</td>\n",
              "      <td>-4.575489e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.206248</td>\n",
              "      <td>-0.112412</td>\n",
              "      <td>-0.320448</td>\n",
              "      <td>-0.029257</td>\n",
              "      <td>0.174161</td>\n",
              "      <td>-0.130551</td>\n",
              "      <td>-0.047241</td>\n",
              "      <td>0.079034</td>\n",
              "      <td>0.103248</td>\n",
              "      <td>0.109671</td>\n",
              "      <td>-0.177046</td>\n",
              "      <td>-0.138711</td>\n",
              "      <td>0.396683</td>\n",
              "      <td>0.542755</td>\n",
              "      <td>0.173605</td>\n",
              "      <td>0.067834</td>\n",
              "      <td>-0.375229</td>\n",
              "      <td>0.084451</td>\n",
              "      <td>0.166926</td>\n",
              "      <td>-0.161879</td>\n",
              "      <td>-0.094449</td>\n",
              "      <td>-0.100859</td>\n",
              "      <td>-0.010304</td>\n",
              "      <td>-0.015311</td>\n",
              "      <td>-0.005672</td>\n",
              "      <td>-0.000221</td>\n",
              "      <td>-6.333343e-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.177131</td>\n",
              "      <td>-0.043059</td>\n",
              "      <td>0.036032</td>\n",
              "      <td>-0.085668</td>\n",
              "      <td>-0.011881</td>\n",
              "      <td>0.217587</td>\n",
              "      <td>0.276693</td>\n",
              "      <td>-0.302752</td>\n",
              "      <td>-0.063586</td>\n",
              "      <td>0.809465</td>\n",
              "      <td>-0.154927</td>\n",
              "      <td>0.117260</td>\n",
              "      <td>-0.011712</td>\n",
              "      <td>-0.104985</td>\n",
              "      <td>0.070532</td>\n",
              "      <td>0.114620</td>\n",
              "      <td>0.084380</td>\n",
              "      <td>-0.001439</td>\n",
              "      <td>0.034631</td>\n",
              "      <td>-0.011631</td>\n",
              "      <td>0.000403</td>\n",
              "      <td>-0.021716</td>\n",
              "      <td>-0.004181</td>\n",
              "      <td>0.001372</td>\n",
              "      <td>0.005900</td>\n",
              "      <td>0.000159</td>\n",
              "      <td>-8.453324e-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.315126</td>\n",
              "      <td>0.064375</td>\n",
              "      <td>0.148327</td>\n",
              "      <td>0.001074</td>\n",
              "      <td>0.056112</td>\n",
              "      <td>-0.059162</td>\n",
              "      <td>-0.018148</td>\n",
              "      <td>0.039909</td>\n",
              "      <td>0.046521</td>\n",
              "      <td>-0.016700</td>\n",
              "      <td>0.042391</td>\n",
              "      <td>-0.012605</td>\n",
              "      <td>-0.026995</td>\n",
              "      <td>-0.005893</td>\n",
              "      <td>-0.101277</td>\n",
              "      <td>-0.044391</td>\n",
              "      <td>0.056174</td>\n",
              "      <td>0.044856</td>\n",
              "      <td>-0.018568</td>\n",
              "      <td>0.084135</td>\n",
              "      <td>-0.111187</td>\n",
              "      <td>-0.175639</td>\n",
              "      <td>-0.092494</td>\n",
              "      <td>-0.810017</td>\n",
              "      <td>0.351740</td>\n",
              "      <td>-0.066199</td>\n",
              "      <td>1.951474e-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.107965</td>\n",
              "      <td>-0.349088</td>\n",
              "      <td>-0.170008</td>\n",
              "      <td>0.132973</td>\n",
              "      <td>0.080431</td>\n",
              "      <td>0.143369</td>\n",
              "      <td>0.027532</td>\n",
              "      <td>0.038043</td>\n",
              "      <td>0.000677</td>\n",
              "      <td>-0.091494</td>\n",
              "      <td>-0.156971</td>\n",
              "      <td>0.037510</td>\n",
              "      <td>-0.383741</td>\n",
              "      <td>-0.297739</td>\n",
              "      <td>-0.213234</td>\n",
              "      <td>0.137543</td>\n",
              "      <td>-0.462905</td>\n",
              "      <td>0.440342</td>\n",
              "      <td>0.099017</td>\n",
              "      <td>0.025707</td>\n",
              "      <td>0.156624</td>\n",
              "      <td>-0.112954</td>\n",
              "      <td>0.003879</td>\n",
              "      <td>0.001573</td>\n",
              "      <td>-0.001255</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>-2.820060e-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.171794</td>\n",
              "      <td>-0.241848</td>\n",
              "      <td>-0.337450</td>\n",
              "      <td>0.032206</td>\n",
              "      <td>0.104250</td>\n",
              "      <td>-0.028708</td>\n",
              "      <td>-0.024814</td>\n",
              "      <td>0.019864</td>\n",
              "      <td>0.104186</td>\n",
              "      <td>-0.096708</td>\n",
              "      <td>-0.261161</td>\n",
              "      <td>-0.059026</td>\n",
              "      <td>0.014375</td>\n",
              "      <td>-0.352975</td>\n",
              "      <td>0.010901</td>\n",
              "      <td>-0.015226</td>\n",
              "      <td>0.235461</td>\n",
              "      <td>-0.140081</td>\n",
              "      <td>-0.248125</td>\n",
              "      <td>-0.316101</td>\n",
              "      <td>-0.526851</td>\n",
              "      <td>0.230079</td>\n",
              "      <td>0.016032</td>\n",
              "      <td>-0.002793</td>\n",
              "      <td>0.003455</td>\n",
              "      <td>-0.002136</td>\n",
              "      <td>4.664310e-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.161012</td>\n",
              "      <td>-0.192984</td>\n",
              "      <td>-0.390056</td>\n",
              "      <td>-0.073205</td>\n",
              "      <td>0.106798</td>\n",
              "      <td>-0.122282</td>\n",
              "      <td>-0.086556</td>\n",
              "      <td>0.203268</td>\n",
              "      <td>0.032182</td>\n",
              "      <td>0.000237</td>\n",
              "      <td>-0.170296</td>\n",
              "      <td>0.232178</td>\n",
              "      <td>0.092323</td>\n",
              "      <td>-0.099240</td>\n",
              "      <td>-0.047643</td>\n",
              "      <td>-0.063027</td>\n",
              "      <td>0.343151</td>\n",
              "      <td>-0.295927</td>\n",
              "      <td>0.059360</td>\n",
              "      <td>0.211429</td>\n",
              "      <td>0.545690</td>\n",
              "      <td>-0.209456</td>\n",
              "      <td>-0.019402</td>\n",
              "      <td>-0.000409</td>\n",
              "      <td>-0.002835</td>\n",
              "      <td>0.004332</td>\n",
              "      <td>-2.641883e-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.315731</td>\n",
              "      <td>0.063837</td>\n",
              "      <td>0.146237</td>\n",
              "      <td>0.001072</td>\n",
              "      <td>0.054704</td>\n",
              "      <td>-0.058453</td>\n",
              "      <td>-0.017680</td>\n",
              "      <td>0.038179</td>\n",
              "      <td>0.045293</td>\n",
              "      <td>-0.014799</td>\n",
              "      <td>0.040013</td>\n",
              "      <td>-0.009155</td>\n",
              "      <td>-0.026978</td>\n",
              "      <td>-0.008018</td>\n",
              "      <td>-0.095525</td>\n",
              "      <td>-0.040316</td>\n",
              "      <td>0.046026</td>\n",
              "      <td>0.026681</td>\n",
              "      <td>0.001109</td>\n",
              "      <td>0.077409</td>\n",
              "      <td>-0.123186</td>\n",
              "      <td>-0.176041</td>\n",
              "      <td>0.010272</td>\n",
              "      <td>-0.028380</td>\n",
              "      <td>-0.507343</td>\n",
              "      <td>0.733402</td>\n",
              "      <td>-3.403166e-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>-0.020787</td>\n",
              "      <td>0.000552</td>\n",
              "      <td>0.358847</td>\n",
              "      <td>0.104230</td>\n",
              "      <td>-0.318296</td>\n",
              "      <td>0.098900</td>\n",
              "      <td>0.024633</td>\n",
              "      <td>-0.059497</td>\n",
              "      <td>0.378774</td>\n",
              "      <td>-0.164126</td>\n",
              "      <td>-0.714320</td>\n",
              "      <td>0.109118</td>\n",
              "      <td>0.101411</td>\n",
              "      <td>0.051240</td>\n",
              "      <td>-0.001062</td>\n",
              "      <td>-0.056416</td>\n",
              "      <td>-0.053987</td>\n",
              "      <td>-0.096383</td>\n",
              "      <td>0.084853</td>\n",
              "      <td>0.117329</td>\n",
              "      <td>0.026166</td>\n",
              "      <td>0.046573</td>\n",
              "      <td>-0.000007</td>\n",
              "      <td>-0.001348</td>\n",
              "      <td>0.000968</td>\n",
              "      <td>-0.002233</td>\n",
              "      <td>1.985849e-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>-0.026625</td>\n",
              "      <td>0.406550</td>\n",
              "      <td>-0.231323</td>\n",
              "      <td>0.002266</td>\n",
              "      <td>0.033230</td>\n",
              "      <td>-0.093736</td>\n",
              "      <td>-0.034139</td>\n",
              "      <td>-0.000673</td>\n",
              "      <td>0.078633</td>\n",
              "      <td>0.060399</td>\n",
              "      <td>-0.130418</td>\n",
              "      <td>-0.003959</td>\n",
              "      <td>-0.205007</td>\n",
              "      <td>0.020827</td>\n",
              "      <td>-0.036677</td>\n",
              "      <td>0.029397</td>\n",
              "      <td>-0.008192</td>\n",
              "      <td>0.025689</td>\n",
              "      <td>0.046134</td>\n",
              "      <td>0.079068</td>\n",
              "      <td>-0.030743</td>\n",
              "      <td>0.019564</td>\n",
              "      <td>0.824934</td>\n",
              "      <td>-0.076478</td>\n",
              "      <td>0.033117</td>\n",
              "      <td>0.003505</td>\n",
              "      <td>-3.847400e-09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>-0.014657</td>\n",
              "      <td>0.415449</td>\n",
              "      <td>-0.139852</td>\n",
              "      <td>0.021969</td>\n",
              "      <td>-0.059398</td>\n",
              "      <td>-0.044296</td>\n",
              "      <td>-0.002289</td>\n",
              "      <td>-0.078446</td>\n",
              "      <td>0.033117</td>\n",
              "      <td>0.007475</td>\n",
              "      <td>-0.200880</td>\n",
              "      <td>-0.026178</td>\n",
              "      <td>-0.337346</td>\n",
              "      <td>0.132786</td>\n",
              "      <td>-0.001059</td>\n",
              "      <td>-0.122509</td>\n",
              "      <td>0.022740</td>\n",
              "      <td>0.040209</td>\n",
              "      <td>-0.314369</td>\n",
              "      <td>-0.527075</td>\n",
              "      <td>0.210057</td>\n",
              "      <td>-0.323368</td>\n",
              "      <td>-0.279898</td>\n",
              "      <td>0.028778</td>\n",
              "      <td>-0.015195</td>\n",
              "      <td>-0.000758</td>\n",
              "      <td>5.863480e-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>-0.016683</td>\n",
              "      <td>0.363815</td>\n",
              "      <td>-0.304982</td>\n",
              "      <td>0.003539</td>\n",
              "      <td>0.088225</td>\n",
              "      <td>-0.130347</td>\n",
              "      <td>-0.047054</td>\n",
              "      <td>0.029074</td>\n",
              "      <td>0.099089</td>\n",
              "      <td>0.102663</td>\n",
              "      <td>-0.104173</td>\n",
              "      <td>-0.007400</td>\n",
              "      <td>-0.300883</td>\n",
              "      <td>0.041342</td>\n",
              "      <td>-0.011110</td>\n",
              "      <td>-0.038980</td>\n",
              "      <td>-0.032670</td>\n",
              "      <td>0.001798</td>\n",
              "      <td>0.237756</td>\n",
              "      <td>0.453331</td>\n",
              "      <td>-0.205683</td>\n",
              "      <td>0.317211</td>\n",
              "      <td>-0.461903</td>\n",
              "      <td>0.042142</td>\n",
              "      <td>-0.013499</td>\n",
              "      <td>-0.001825</td>\n",
              "      <td>2.692188e-09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>-0.097204</td>\n",
              "      <td>0.364488</td>\n",
              "      <td>0.002754</td>\n",
              "      <td>-0.117452</td>\n",
              "      <td>0.069379</td>\n",
              "      <td>-0.187568</td>\n",
              "      <td>-0.075973</td>\n",
              "      <td>-0.043662</td>\n",
              "      <td>0.073291</td>\n",
              "      <td>-0.049502</td>\n",
              "      <td>-0.047270</td>\n",
              "      <td>0.114468</td>\n",
              "      <td>0.437870</td>\n",
              "      <td>-0.392432</td>\n",
              "      <td>-0.125096</td>\n",
              "      <td>0.593184</td>\n",
              "      <td>-0.032893</td>\n",
              "      <td>0.170023</td>\n",
              "      <td>0.018285</td>\n",
              "      <td>-0.063368</td>\n",
              "      <td>0.031020</td>\n",
              "      <td>-0.095917</td>\n",
              "      <td>-0.123178</td>\n",
              "      <td>0.009993</td>\n",
              "      <td>-0.006313</td>\n",
              "      <td>-0.000854</td>\n",
              "      <td>5.554479e-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.158012</td>\n",
              "      <td>-0.182320</td>\n",
              "      <td>-0.153313</td>\n",
              "      <td>-0.069562</td>\n",
              "      <td>-0.450291</td>\n",
              "      <td>-0.061228</td>\n",
              "      <td>0.068617</td>\n",
              "      <td>-0.216006</td>\n",
              "      <td>0.140286</td>\n",
              "      <td>-0.123714</td>\n",
              "      <td>0.173357</td>\n",
              "      <td>-0.035038</td>\n",
              "      <td>-0.240388</td>\n",
              "      <td>0.340517</td>\n",
              "      <td>-0.244859</td>\n",
              "      <td>0.550294</td>\n",
              "      <td>0.170856</td>\n",
              "      <td>-0.140248</td>\n",
              "      <td>0.021230</td>\n",
              "      <td>-0.019915</td>\n",
              "      <td>-0.016786</td>\n",
              "      <td>0.052566</td>\n",
              "      <td>-0.005083</td>\n",
              "      <td>0.000262</td>\n",
              "      <td>0.002661</td>\n",
              "      <td>0.000475</td>\n",
              "      <td>6.006124e-11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>-0.012531</td>\n",
              "      <td>0.042679</td>\n",
              "      <td>-0.008792</td>\n",
              "      <td>-0.017729</td>\n",
              "      <td>-0.002105</td>\n",
              "      <td>-0.277493</td>\n",
              "      <td>0.899846</td>\n",
              "      <td>0.251889</td>\n",
              "      <td>-0.134964</td>\n",
              "      <td>-0.155551</td>\n",
              "      <td>-0.063583</td>\n",
              "      <td>0.000424</td>\n",
              "      <td>0.019122</td>\n",
              "      <td>-0.003258</td>\n",
              "      <td>-0.003491</td>\n",
              "      <td>-0.000860</td>\n",
              "      <td>-0.018787</td>\n",
              "      <td>-0.004840</td>\n",
              "      <td>0.000458</td>\n",
              "      <td>0.007829</td>\n",
              "      <td>-0.009393</td>\n",
              "      <td>0.009064</td>\n",
              "      <td>-0.000333</td>\n",
              "      <td>-0.000245</td>\n",
              "      <td>-0.000218</td>\n",
              "      <td>-0.000114</td>\n",
              "      <td>1.816505e-11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.008889</td>\n",
              "      <td>0.036418</td>\n",
              "      <td>-0.020738</td>\n",
              "      <td>-0.480513</td>\n",
              "      <td>0.280038</td>\n",
              "      <td>0.343089</td>\n",
              "      <td>0.164173</td>\n",
              "      <td>-0.231109</td>\n",
              "      <td>0.259512</td>\n",
              "      <td>-0.209116</td>\n",
              "      <td>-0.015493</td>\n",
              "      <td>-0.565601</td>\n",
              "      <td>-0.025839</td>\n",
              "      <td>-0.090178</td>\n",
              "      <td>0.077398</td>\n",
              "      <td>0.012150</td>\n",
              "      <td>0.043226</td>\n",
              "      <td>-0.079909</td>\n",
              "      <td>0.049810</td>\n",
              "      <td>0.092708</td>\n",
              "      <td>0.144284</td>\n",
              "      <td>-0.066630</td>\n",
              "      <td>-0.005202</td>\n",
              "      <td>0.001305</td>\n",
              "      <td>-0.001208</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>-3.214525e-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>-0.100476</td>\n",
              "      <td>0.004307</td>\n",
              "      <td>0.204630</td>\n",
              "      <td>0.164025</td>\n",
              "      <td>0.507888</td>\n",
              "      <td>0.258302</td>\n",
              "      <td>-0.043357</td>\n",
              "      <td>0.170753</td>\n",
              "      <td>-0.369845</td>\n",
              "      <td>-0.061903</td>\n",
              "      <td>-0.289305</td>\n",
              "      <td>0.001581</td>\n",
              "      <td>-0.147571</td>\n",
              "      <td>0.297041</td>\n",
              "      <td>-0.144771</td>\n",
              "      <td>0.388129</td>\n",
              "      <td>0.196105</td>\n",
              "      <td>-0.127507</td>\n",
              "      <td>-0.035020</td>\n",
              "      <td>0.008167</td>\n",
              "      <td>-0.054034</td>\n",
              "      <td>0.022239</td>\n",
              "      <td>-0.006971</td>\n",
              "      <td>0.000723</td>\n",
              "      <td>0.000155</td>\n",
              "      <td>0.000320</td>\n",
              "      <td>-2.595122e-11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2   ...        24        25            26\n",
              "0  -0.000956 -0.009323 -0.014329  ... -0.000278  0.000866 -1.864768e-10\n",
              "1   0.023684  0.016026 -0.076667  ... -0.000731  0.000496 -8.554078e-11\n",
              "2   0.316230  0.063195  0.142777  ... -0.580915 -0.676503 -2.643839e-09\n",
              "3   0.302322  0.038265  0.084283  ... -0.009645  0.002023 -2.944233e-10\n",
              "4   0.319960  0.036528  0.113152  ...  0.480526  0.004579  1.143248e-08\n",
              "5   0.287480  0.117286  0.183743  ...  0.208895 -0.000760  5.484149e-09\n",
              "6   0.275741  0.104237  0.032984  ...  0.005740 -0.000064  4.878355e-11\n",
              "7   0.280154  0.064794  0.163190  ...  0.071376  0.003063  6.289658e-10\n",
              "8  -0.193227 -0.191122  0.217187  ...  0.000504 -0.000091 -7.149193e-01\n",
              "9   0.211613  0.128547 -0.119615  ...  0.001237  0.000430 -5.180564e-01\n",
              "10  0.001045  0.176596 -0.166221  ... -0.002493 -0.000342 -4.575489e-01\n",
              "11  0.206248 -0.112412 -0.320448  ... -0.005672 -0.000221 -6.333343e-02\n",
              "12  0.177131 -0.043059  0.036032  ...  0.005900  0.000159 -8.453324e-02\n",
              "13  0.315126  0.064375  0.148327  ...  0.351740 -0.066199  1.951474e-08\n",
              "14  0.107965 -0.349088 -0.170008  ... -0.001255 -0.000217 -2.820060e-10\n",
              "15  0.171794 -0.241848 -0.337450  ...  0.003455 -0.002136  4.664310e-10\n",
              "16  0.161012 -0.192984 -0.390056  ... -0.002835  0.004332 -2.641883e-10\n",
              "17  0.315731  0.063837  0.146237  ... -0.507343  0.733402 -3.403166e-08\n",
              "18 -0.020787  0.000552  0.358847  ...  0.000968 -0.002233  1.985849e-10\n",
              "19 -0.026625  0.406550 -0.231323  ...  0.033117  0.003505 -3.847400e-09\n",
              "20 -0.014657  0.415449 -0.139852  ... -0.015195 -0.000758  5.863480e-10\n",
              "21 -0.016683  0.363815 -0.304982  ... -0.013499 -0.001825  2.692188e-09\n",
              "22 -0.097204  0.364488  0.002754  ... -0.006313 -0.000854  5.554479e-10\n",
              "23  0.158012 -0.182320 -0.153313  ...  0.002661  0.000475  6.006124e-11\n",
              "24 -0.012531  0.042679 -0.008792  ... -0.000218 -0.000114  1.816505e-11\n",
              "25  0.008889  0.036418 -0.020738  ... -0.001208  0.000500 -3.214525e-10\n",
              "26 -0.100476  0.004307  0.204630  ...  0.000155  0.000320 -2.595122e-11\n",
              "\n",
              "[27 rows x 27 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 886
        },
        "id": "yeWSNKrIvytE",
        "outputId": "06f8d7c6-5bc7-4794-d48f-00aceb983846"
      },
      "source": [
        "expLianedVariance=(pca.explained_variance_/pca.explained_variance_.sum())*100\n",
        "expLianedVarianceCum=expLianedVariance.cumsum()\n",
        "pd.DataFrame( expLianedVarianceCum, expLianedVariance)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3.361540e+01</th>\n",
              "      <td>33.615396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.800668e+01</th>\n",
              "      <td>51.622078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9.966979e+00</th>\n",
              "      <td>61.589057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6.808087e+00</th>\n",
              "      <td>68.397144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5.912362e+00</th>\n",
              "      <td>74.309506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4.503003e+00</th>\n",
              "      <td>78.812509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3.713058e+00</th>\n",
              "      <td>82.525567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2.937357e+00</th>\n",
              "      <td>85.462924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2.833553e+00</th>\n",
              "      <td>88.296477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2.546834e+00</th>\n",
              "      <td>90.843311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2.263997e+00</th>\n",
              "      <td>93.107308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.640678e+00</th>\n",
              "      <td>94.747986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.126744e+00</th>\n",
              "      <td>95.874730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9.472433e-01</th>\n",
              "      <td>96.821973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8.798040e-01</th>\n",
              "      <td>97.701777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7.051214e-01</th>\n",
              "      <td>98.406899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4.740928e-01</th>\n",
              "      <td>98.880992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4.082128e-01</th>\n",
              "      <td>99.289204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3.177380e-01</th>\n",
              "      <td>99.606942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.516145e-01</th>\n",
              "      <td>99.758557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.345332e-01</th>\n",
              "      <td>99.893090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.015490e-01</th>\n",
              "      <td>99.994639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4.241735e-03</th>\n",
              "      <td>99.998881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8.324112e-04</th>\n",
              "      <td>99.999713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2.470488e-04</th>\n",
              "      <td>99.999960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3.982252e-05</th>\n",
              "      <td>100.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3.068240e-17</th>\n",
              "      <td>100.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       0\n",
              "3.361540e+01   33.615396\n",
              "1.800668e+01   51.622078\n",
              "9.966979e+00   61.589057\n",
              "6.808087e+00   68.397144\n",
              "5.912362e+00   74.309506\n",
              "4.503003e+00   78.812509\n",
              "3.713058e+00   82.525567\n",
              "2.937357e+00   85.462924\n",
              "2.833553e+00   88.296477\n",
              "2.546834e+00   90.843311\n",
              "2.263997e+00   93.107308\n",
              "1.640678e+00   94.747986\n",
              "1.126744e+00   95.874730\n",
              "9.472433e-01   96.821973\n",
              "8.798040e-01   97.701777\n",
              "7.051214e-01   98.406899\n",
              "4.740928e-01   98.880992\n",
              "4.082128e-01   99.289204\n",
              "3.177380e-01   99.606942\n",
              "1.516145e-01   99.758557\n",
              "1.345332e-01   99.893090\n",
              "1.015490e-01   99.994639\n",
              "4.241735e-03   99.998881\n",
              "8.324112e-04   99.999713\n",
              "2.470488e-04   99.999960\n",
              "3.982252e-05  100.000000\n",
              "3.068240e-17  100.000000"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dlLyXtcv190",
        "outputId": "c237a19e-c579-4512-8ccf-c33e7f9f956f"
      },
      "source": [
        "expLianedVariance.size"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "Lypn6zs8v8Hc",
        "outputId": "bb1153e2-1f85-4a7f-a99f-f6d3ebd8a571"
      },
      "source": [
        "xlabel=range(1, expLianedVariance.size+1)\n",
        "xlabel=np.reshape( np.asarray(list(xlabel)), (27,))\n",
        "N_PC_sel=5\n",
        "variance_N_PC= expLianedVarianceCum[N_PC_sel-1]\n",
        "plt.figure( )\n",
        "\n",
        "plt.plot(list(xlabel),expLianedVariance, label='Comp. Variation')\n",
        "plt.plot(list(xlabel),expLianedVarianceCum, label = 'Cum. Variation', linewidth=3)\n",
        "#Vertical line at the selected no of components\n",
        "plt.axvline(x=N_PC_sel, linestyle='dashed',linewidth=.5, color='grey')\n",
        "# add apoint\n",
        "plt.plot(N_PC_sel,variance_N_PC, color='green', marker='o')\n",
        "plt.text(N_PC_sel,variance_N_PC,\"{0:.0%}\".format(variance_N_PC/100.0), horizontalalignment='right')\n",
        "\n",
        "plt.legend()\n",
        "plt.title('PCA')\n",
        "plt.xlabel('Number of principal components')\n",
        "plt.ylabel('Expalined Variation')\n",
        "plt.ylim([0,100])\n",
        "plt.xlim([0,20])"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 20.0)"
            ]
          },
          "metadata": {},
          "execution_count": 73
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "KJogCbk2wFJA",
        "outputId": "6e0ff208-c7c9-4cde-d47d-90cef3a67c2c"
      },
      "source": [
        "pd.DataFrame(pca.components_.T).loc[:4,:5]"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.000956</td>\n",
              "      <td>-0.009323</td>\n",
              "      <td>-0.014329</td>\n",
              "      <td>0.605191</td>\n",
              "      <td>-0.079737</td>\n",
              "      <td>-0.240022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.023684</td>\n",
              "      <td>0.016026</td>\n",
              "      <td>-0.076667</td>\n",
              "      <td>0.388179</td>\n",
              "      <td>0.375549</td>\n",
              "      <td>0.017731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.316230</td>\n",
              "      <td>0.063195</td>\n",
              "      <td>0.142777</td>\n",
              "      <td>0.000632</td>\n",
              "      <td>0.055973</td>\n",
              "      <td>-0.059354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.302322</td>\n",
              "      <td>0.038265</td>\n",
              "      <td>0.084283</td>\n",
              "      <td>-0.009740</td>\n",
              "      <td>-0.077429</td>\n",
              "      <td>-0.045670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.319960</td>\n",
              "      <td>0.036528</td>\n",
              "      <td>0.113152</td>\n",
              "      <td>0.005853</td>\n",
              "      <td>0.029840</td>\n",
              "      <td>-0.063156</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2         3         4         5\n",
              "0 -0.000956 -0.009323 -0.014329  0.605191 -0.079737 -0.240022\n",
              "1  0.023684  0.016026 -0.076667  0.388179  0.375549  0.017731\n",
              "2  0.316230  0.063195  0.142777  0.000632  0.055973 -0.059354\n",
              "3  0.302322  0.038265  0.084283 -0.009740 -0.077429 -0.045670\n",
              "4  0.319960  0.036528  0.113152  0.005853  0.029840 -0.063156"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "9R9njtgxwI2k",
        "outputId": "78567aaf-e8ad-4acf-bcba-1252884eebc9"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "n = len(X_reduced)\n",
        "kf_10 = KFold( n_splits=10, shuffle=True, random_state=1)\n",
        "\n",
        "regr = LinearRegression()\n",
        "mse = []\n",
        "\n",
        "score = -1*cross_val_score(regr, np.ones((n,1)), y.ravel(), cv=kf_10, scoring='neg_mean_squared_error').mean()    \n",
        "mse.append(score)\n",
        "\n",
        "for i in np.arange(1, 28):\n",
        "    score = -1*cross_val_score(regr, X_reduced[:,:i], y.ravel(), cv=kf_10, scoring='neg_mean_squared_error').mean()\n",
        "    mse.append(score)\n",
        "       \n",
        "plt.plot(mse, '-v')\n",
        "plt.xlabel('Number of principal components in regression')\n",
        "plt.ylabel('MSE')\n",
        "plt.title('popother')\n",
        "plt.xlim(xmin=-1);"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXycZbn/8c93sk2XTNckLV0IS9oCFUppS0GWQmUREVS2IgqcA1RRLLic43I4ir70qKigyA+1CiJbAVG0AooIZZGlbbpv0JbSldKkdEtpmzbJ9fvjedJO02zTzmQyM9f79cqrzzzbXM/MdK657/u571tmhnPOudwVSXcAzjnn0ssTgXPO5ThPBM45l+M8ETjnXI7zROCccznOE4FzzuU4TwTOdQBJ10r6d7rjcK45ngicSzJJ5ZJMUn66Y3GuPTwROJdhPMG4ZPNE4LKepJWSvilpsaTNkn4vKRpuu0HSckmbJE2VdFjccSZpkqQVkjZK+omkSLgtIulWSaskVUl6QFKP8NCXw3+3SNou6ZS4c/40jOEdSR+NW99D0r2S1ktaJ+n7kvLCbddKelXSnZLeB25L7Svmco0nApcrrgLOA44ChgC3Sjob+CFwOdAfWAU82uS4TwKjgJHAxcB/huuvDf/OAo4EugN3h9vOCP/taWbdzez18PHJwFtAX+B24F5JCrfdD9QBRwMnAucC18fFcTKwAigDfpD45TvXCjPLuD/gPqAKWNiOfQcD04A5wHzggnTH738d+wesBD4f9/gC4G3gXuD2uPXdgT1AefjYgPPjtn8BeD5cfh74Qty2oeGx+UB5eGx+3PZrgeVxj7uG+/Qj+HKvBbrEbb8SmBZ37Op0v47+l71/mVoiuB84v5373go8bmYnAhOAe1IVlOvU1sQtrwIOC/9WNa40s+3A+8CANo6j6bHhcj7Bl3pL3ot7rh3hYnfgcKAAWC9pi6QtwG+A0hbicC6pMjIRmNnLwKb4dZKOkvQPSbMkvSJpWOPuQCxc7gG824Ghus5jUNzyYILPwbsEX8IASOoG9AHWtXEcTY8Nt9UBGwg+c4lYQ1Ai6GtmPcO/mJkdF7ePDxPsUiYjE0ELJgNfMrOTgK+x75f/bcBnJK0FngG+lJ7wXJp9UdJASb2B/wEeA6YA/yFphKQi4P+A6Wa2Mu64/5LUS9Ig4ObwOMJjvyzpCEndw2MfM7M6oBpoIGg7aJOZrQf+CfxMUixsiD5K0pmHfNXOtUNWJILwP+KpwB8lzSUoVvcPN18J3G9mAwnqhh9svPPD5ZRHCL5sVxC0D3zfzP4F/C/wJ2A9QUPyhCbH/RWYBcwFniZoV4CgnepBgjuE3gF2Ef7ICKt9fgC8Glb1jG1HfFcDhcBiYDPwBPs+w86llMwys8QpqRx4ysyGS4oBb5nZAf9xJC0iaPBbEz5eAYw1s6qOjNelj6SVwPXhF38ixxlQYWbLUxKYc51EVvwyNrNtwDuSLgNQ4IRw82pgfLj+GCBKUHR3zjlHhiYCSVOA14GhktZKuo7gPvHrJM0DFhHc8w3wVeCGcP0U4FrL1GKQc86lQMZWDTnnnEuOlJcIJOVJmiPpqWa2FUl6LOziPz2s93fOOdeBOmLwqpuBJey7lz/edcBmMzta0gTgx8AVrZ2sb9++Vl5envQgnXMum82aNWujmZU0ty2liUDSQOBjBLfSfaWZXS5m3wBaTwB3S1Jrdfjl5eVUVlYmO1TnnMtqkla1tC3VVUM/B/6boHNNcwYQdp0PO+JsJejZuR9JEyVVSqqsrvYbfpxzLplSlggkXQhUmdmsQz2XmU02s1FmNqqkpNmSjXPOuYOUyhLBh4GLws48jwJnS3qoyT7rCMdyCSfb6EEw6JdzzrkOkrJEYGbfNLOBZlZO0G3/BTP7TJPdpgLXhMuXhvv4/azOOdeBOnzKO0nfAyrNbCrBuC0PSlpOMJpo03FenHPOpViHJAIzexF4MVz+dtz6XcBlHRGDS40LfvEKi9dvO2D9sf1jPHPz6WmIyDmXqIwcYsJ1HiMH96QgT/utK8gTIw/vlaaInHOJ8kTgDsmk8RVEtH8iyJOYNP7oNEXknEuUJwJ3SEpjUU4+ovfexwV54tJRgygtjqYxKudcIjwRuEOy+N1tzHhnv1lDvTTgXIbxROAO2sbttdzwQCU9uxbyqZHBfO9F+RF6dS1Mc2TOuUR4InAHZXddA194aDYbt9cy+eqT+Mb5wxhS1p3ttfX8adbadIfnnEuAJwKXMDPjO1MXMWPlJm6/9HiOH9iT0liUZ285gxGDenLX88uoratPd5jOuXbyROAS9tAbq5gyYzVfGHcUF48YsHe9JL527lDe3bqLR2esSWOEzrlEeCJwCXlt+UZu+9tiPnJMKV87d+gB2z98dB9OPqI3d09bzs7dXipwLhN4InDttvr9HXzhkdkc2bcbd14xgkhEB+wjia+dN5TqmloeeH1lh8fonEucJwLXLttr67j+gZmYwW+vHkVxtKDFfUeX9+bMISX8+qW3qdm1pwOjdM4dDE8Erk0NDcYtj87l7eoPuOeqkZT37dbmMV89dwibd+zhvn+vTH2AzrlD0uGjj3Y2Pmha2+54bin/WrKB2z5+LB8+um+7jjl+YE/OO66M372ygmtOPZye3rfAuU4r5xPByME9WVZVw576fdMgtDRoWqYmjUTibmnfnl0LuObU8oSe98vnDOGfi19h8ssr+O/zhyV0rHOu4+R81dANZxxJ06lw6hqMdzfv4H//spC7nl/GI9NX89ziDQzu3SUjR9pMZITQ5vYVcMHw/kgHNg63Zli/GB8//jB+/+pKNm6vTThu51zHyNkSwdrNO3jw9VU8OnMNdQ37MoGA0u5FrN68k9lrtrBlR+uNnZkw0uak8RX8cdZaYP+Md1RJN6bMWM3uugZq6+rZXddAYX6EhiaJsTA/wi3nVBzUc9/ykQqeXrCee6a9zbc/fuxBXoFzLpVSlggkRYGXgaLweZ4ws+802eda4CcEcxcD3G1mv0tVTGbGjHc2cf9rK3l20XtI4rzjyvjEiAF8acocausaKMqP8LdJp+0dPXN3XQPvf1BLdU1tMJzCSyuYuXITjTVJZw4t6fQjbZbGolw84jAer9w39MOeeuO7f1t8wL5SkAwb5UfEZYcwmuiRJd25ZOQAHpq+ihvOOIL+Pboc1Hmcc6mTyhJBLXC2mW2XVAD8W9LfzeyNJvs9ZmY3JfOJW6rnLsqPUFvXQI8uBUw84yg+e8rhDOgZfDFddtJAHp6x+oAhlAvzI/Tv0WXvF9jww3pw+u3TqK9rQMAryzYye/VmRg7uvNVDZsamD3bvfVyYF+GB68bQLxalqCBCYV6EooI8CvMiFOSJ6ppaTr99GrV1DeRHDr3E86WzK3hyzjrufmE5P/jkhw71cpxzSZbKyevNzLaHDwvCvw6ZmL65em6AaEGEH37qQ7zxzfF846PD9iYBCKpPRpf3bvNLrzQW5bKTBiLBJ0cOoKS4iGvuncHcNVuSfh3J8vD01fxrSRUjBvVAgstHD2LskX0o79uN/j260Kd7Ed2L8inMjyBpv2tMxtwCg3p3ZcLowTw2cw1rNu1I0lU555IlpY3FkvIkzQWqgOfMbHozu10iab6kJyQNSsbzThpfgdg/ERTmiee+fCZXjhlMl8K8A44pjUV5/HOntOtLrzFpfOOjw5hyw1h6dSvks/dOZ14nTAYL123le08t5owhJfz6qpPaleyg/YmxvW46+2jyIuLn/1qWlPM555InpYnAzOrNbAQwEBgjaXiTXf4GlJvZ8cBzwB+aO4+kiZIqJVVWV1e3+bylsSiXjRpI4wgIBXni8tGDKY0lpy4/Pmkc1rMLUyaOpWfXAj5z73Tmr+08yWDbrj188ZHZ9O5ayJ2Xn0C/nl3anewSSYztURaLcvUph/PknLUsr9re9gHOuQ4ja3rvZKqeSPo2sMPMftrC9jxgk5n1aO08o0aNssrKyjafr2rbrr313NH8CC9//ayUNuqu27KTCZNfZ+uOPTx8/Vg+NLDVy0g5M+OmR+bwj0Xv8ejEsYwu7932QSl27p0vsXTDgUmgs/fDcC4bSJplZqOa25ayEoGkEkk9w+UuwDnAm0326R/38CJgSbKeP9n13G0Z0LMLU24YS6xLUDJYuG5rSp+vLQ++sYqnF6zna+cO7RRJAGBMeW+ajlOXCf0wnMt2qawa6g9MkzQfmEnQRvCUpO9JuijcZ5KkRZLmAZOAa5MZQLLrudsysFdXptwwlu5F+Vz1u/QlgwVrt/L9p5Zw1tASPnfGkWmJoTmTxldQkLf/Ry4T+mE4l+06rGooWdpbNZROazbtYMLkN1i/decBnbMgsaEdEq022bpzDxf+8hXq642nJ51Or26da4yfW59cwJQZq6m3oI/ChDGD+f4nmjYdOeeSLS1VQ7lsUO+gZBAtOPDupESGdki02sTM+PoT81m/ZRe//PTITpcEICgV5IelggYzLw041wnk7BATqTa4T1ceum4Mn/rV6/utN4N+sSIefH0lkYiISORJHNG32wFjHiVabXL/ayv5x6L3+NYFwzipk9a7N7bdPDR9NQ0WlJ46e89s57KdJ4IUGnl4by46oT9T563fu66uwfjpP5e26/jyvt3YWLO7XV+Uc9ds4f+eWcJHjinlhtM7T7tAcyaNr+DN92pYUb2dO55bysPXj013SM7lNG8jSLH421iL8iM8Pek0enUtpMGCqpEGM+obDLNg3yt/O53d9Q1EBBFBXQMc0z/GJSMHcPGIoCdzU1t37OGCu14B4OlJp2XM2P+/e2UF3396CY9OHMvYI/ukOxznslprbQSeCDrArU8u4OEZq7nq5MPbbBiN3/er5wzhb/Pf5U+z1jJv7VbyImLckBIuOWkgv3xhGUvW1xxwfCbdk79rTz1n3D6N8r7deGzi2ISHuXbOtV9ricCrhjrApPEVLK3a3u6hHRr37dWtkKtPKefqU8pZtqGGP81ex5Nz1vL8m1UU5omI2O+upEy7Jz9akMcXzzqa70xdxGtvv9/u2c+cc8nlJYIMU99gvLp8Iw9PX8Wzizbst60jelAnW21dPWf95EX69YjypxtP9VKBcynit49mkbyIOGNICb/57CguHzWQvLCrbkGeOqQHdbIV5edx09kVzF69hReXtj2OlHMu+TwRZLCvnTuU/DARZHIP3UtPGsjAXl2487mlZFoJ1bls4Ikgg3X0eEqpUpgfYdL4Cuav3cq/llSlOxznco4nggzX0eMppcqnThxAeZ+u3PHcUhqaG5fDOZcynggyXLLnDUiX/LwIN3+kgiXrt/HsovfSHY5zOcUTges0LjphAEeVdOPOf3mpwLmO5InAdRp5EXHLR4awdMN2nlqwvu0DnHNJ4YnAdSof+1B/hpYV8/N/LaXeSwXOdQhPBK5TiUTEl8+pYEX1B/x17rp0h+NcTvBE4Dqdc4/tx7H9Y/zi+WXsqW9IdzjOZb1UzlkclTRD0rxwOsrvNrNPkaTHJC2XNF1SearicZkjEhFfOWcIq97fwZOzvVTgXKqlskRQC5xtZicAI4DzJTUdeP46YLOZHQ3cCfw4hfG4DDL+mFJOGNiDXzy/jN11XipwLpVSNvqoBWMFbA8fFoR/TVv/LgZuC5efAO6WJPNxBnKeJDbv2MO6LTsZcuvf99uWSUNtO5cJUtpGIClP0lygCnjOzKY32WUAsAbAzOqArcABM5RImiipUlJldbUPTJYrzqjoS9OxSDNtqG3nMkFK5yMws3pghKSewJOShpvZwoM4z2RgMgTDUCc5TNdJTRpfwWOVa9hTv+8tj6RhcL0LfvEKi9dvO2B9cyWTRPZNZRzOJaJDJqYxsy2SpgHnA/GJYB0wCFgrKR/oAbzfETG5zq80FuWKUYOYMnM1jTcP1dY1cMMfKvn4CYfxseP7079Hl4TPm+gX6sjBPVlWVbNfQmqpZJLIvonGkui5nWuvlCUCSSXAnjAJdAHO4cDG4KnANcDrwKXAC94+4OJNGl/BH2etpb4hmPP5c2ceybQ3q/n+00v4/tNLGFPemwtP6M9Db6xi6YbtBxx/sF+odfUNbN25hy079zBuaAmPVa7Z7xwNBlt37ubGh2ZRs6uOml17qNlVx5Yde/Y7b3AuY8m727jl0TmUxaKUxqKUxYroF4sypKz7AbHkR0TvbgX85qW3eXfLTtZt2cX6rTtZu3nHAec2g2tPPTzxF9a5OCmboUzS8cAfgDyCtojHzex7kr4HVJrZVElR4EHgRGATMMHMVrR23lyfoSwXNTfn8zsbP+Cpee8ydd67LKsKEoDY/26E/Ig4c0gJV4wexM499dTuaWDnnno21tRyz0tv79dzOSIY2q+YD2rr2bxjNzW76lqNKS8CfbsXURwtoDiav/ffWDSfuWu2sPS9GuotOO+g3l0p6V7EhppdbNhWm9BdUMXRfAb07MJhPbvQv0eURe9uZcG6rcR3ryjIE2cNLeWTJw7g7GNKKcrPa/f5Xe7wyetdRqvatoubpszh7k+f2Owoq2+9V8OjM1Zz/2srD7gtrb36dCtk+IAe9OpaQM+uhfTsWkDPLvuWGww+/+Asdtc3tDklaNW2XZx++zRq6w7c18zYunMPG7bVsmHbLt7btouH31jFgnVbabAgwZxZUcI3LjiG/j2iFEcLWj33b68ZxUtvVfPXee9SXVNLLJrPx47vzydPHMhtUxd1ijYFb9voHHzyepfRGofabsnQfsV856Lj2FPfwKMz11DXYORFxLghJXx+3FF0KcgjWpBHtCBCl4I8uhTmsXXHHsb99MW9X6h/v+X0NofyvnzUQB6esbrNSYAaJwxqbl9JYXIpZGi/YgDGDSnZ++VeEInwo0uPb/H8Tc99ekUJp1eU8M0LjuHV5Rv5y5x1/HXuu0yZsYZuhXlEFFRjNUpHm4K3bXR+PsSEyxqTxlfsm8M5In54yYcYXd6b4QN6cHRpdwb26kqf7kV0Lcynf88uCc/ulsgkQInsm+hMc82du3Eu6zuuGEHlrR/h51eMYPjAHjQdty8dU5pOGl9BRPvfCJzJU6tmI68aclmlufaElrRV5dSRUhXLVx+fy5Nz1u1NCOOHlXLvtaOTdv62LK+q4fevruTRGauJb+cuyo9wekUJIw/vycjBvTh+YA+6FuZ7NVIKeRuByxmd6cu9M4hvU2hsTJ8wehDf+tgxxJq0PyRLQ4Px8rJq7nt1JS8vraYwP8J5x5bx7KIN7K5vID8izjuuH0vWb2PFxg+AoEQzrF8xu+saWLFx+wGN4VeMHtxmYnet8zYClzPaak/INfFtChNGD6JHl0Imv/w2Ly2t5keXHM+ZQ0oSPmdLv9qH9SvmqrGHc/+r7/B29QeUFBfx1XOG8OmTB9One9He0tqEMfu+1Dd/sJu5a7YwZ/VmZq/ewuzVm2k64KxXI6WeJwLnstyk8RUsrdrOl88ZQmlxlPOOK+O/npjPNffN4IpRg/ifCxMrHTTX+BsRvF29nf/9y0I+NKAHd15xAh/70GEU5u9rhmyMI/5LvVe3Qs4aVspZw0oBqG8wbp4yh78vWr83IYwY1JOS7kWH+Cq41njVkHM5aNeeen7x/DJ+89LblMWiFORFWL1pxwH7NVc3v+r9DzjnjpfZ3eSn+/hhJdw47mhOOrwXUtNRotovvjqr8a6nT40cwPc/MZyuhf7b9WB51ZBzbj/Rgjy+fv4wzjuuH//1x3ksq9re7K2mR5d255kF63nzvRreXL+NtzbUsHrTDuJ/P0YEF484jDuvODEpscVXZ105ZjB9uxdx1wvLWLhuK/dcdRJHl3ZPyvO4fbxE4FyO27Wnnh8+s4Q/vL6qxX0igvK+3RjWr5ihZTH694hy618XsruZTnPJ0LTR/+Wl1dzy2Fxq99Tzw0uO56ITDkvac+UKLxE451oULcjjuxcPZ0NNLf9Y+B4QDNcxtF8x1512BMP6xago6060YP+hK+av3dKuDnYHo2mj/xlDSnh60ml86ZE5TJoyh5nvbOLWC4/x4TSSxBOBcw6A7110HNPerKK2Lhjg74HrxrT6Bd9c428q9e/RhSkTx/KTZ99i8ssrmLd2Czt217O8qn2DDXofhZZ5z2LnHJB4D+fGX+0d2V+jIC/Cty44ht989iTe2fgBq97/YG9v8n37tDxEeEFe+/bNNV4icM7t1dG/8g/Wecf145h+Ma5/YGazw4+PGtyLaW9W0TjFnYBR5b0OGE7c+ygEvLHYOZexdu2p58Jf/rvZ6qG25FqPZW8sds5lpWhBHo9cfzKn/Xgau+sbKMyL8JvPnkTPrvs6yDX+1DULejLf+PAs9tQbdfXmk/qEPBE45zJaaSy6d4jwy0cP2ttLuSVXjBrEw9NXI+Abf1rAQ9effMAdUbnGG4udcxkv4SHCj+jN9z85nFmrNzNpypz9ZqvLRamcqnIQ8ABQRlA6m2xmv2iyzzjgr8A74ao/m9n3WjuvtxE455Ll/lff4ba/LebTJw/mB58YfkhDY3R26WojqAO+amazJRUDsyQ9Z2aLm+z3ipldmMI4nHOuWdd++Ag21NTyqxffpl8syqTxFekOKS1SlgjMbD2wPlyukbQEGAA0TQTOOZc2/33eUDZs28Udzy2ltLiICWMGpzukDtchbQSSyoETgenNbD5F0jxJf5d0XAvHT5RUKamyuro6hZE653KNJH4czs3wrScX8NziDekOqcOlPBFI6g78CbjFzJr2754NHG5mJwC/BP7S3DnMbLKZjTKzUSUliU+k4ZxzrSnIi3DPVSMZPqAHNz0ym1mrNqU7pA6V0kQgqYAgCTxsZn9uut3MtpnZ9nD5GaBAUt9UxuScc83pVpTPfdeOpn+PKNf9oZLlVTXpDqnDpKyNQEHz+73AEjO7o4V9+gEbzMwkjSFITO+nKibnnGtN3+5FPPCfJ3PWz6bxkTtePmB7tg5Ql8q7hj4MfBZYIGluuO5bwGAAM/s1cClwo6Q6YCcwwTJtzAvnXFYZ3Kcr5x7bj7+HQ3I3yuYB6lJ519C/2TvkU4v73A3cnaoYnHPuYHz3ouP415IN+83LnM0D1HnPYueca6I0FuWKUYNoHLU6P08pmYCns/BE4JxzzZg0voJIONeBIGtLA+CJwDnnmlUai/LR4f0AGF3eO2tLA+CJwDnnWvSVc4YCMGJwzzRHklqeCJxzrgXlfbtRXJTPjtr6dIeSUp4InHOuFaWxIjZs25XuMFLKE4FzzrWiLBb1ROCcc7ksSAS16Q4jpTwROOdcK0pjRVTX1JLNgx54InDOuVaUFUfZXd/Alh170h1KyngicM65VpTFgv4DG2qyt53AE4FzzrWiLFYEkNXtBJ4InHOuFXtLBFl855AnAueca0VJcVAiqPJE4JxzuSlakEfPrgVeNeScc7msrDi7O5W1mggkfSZu+cNNtt2UqqCcc64zKY0VsaEmd0sEX4lb/mWTbf/Z2oGSBkmaJmmxpEWSbm5mH0m6S9JySfMljWxn3M4512HKYtGsbiNoa6pKtbDc3OOm6oCvmtlsScXALEnPmdniuH0+ClSEfycDvwr/dc65TqMsVkRVTS0NDbZ3spps0laJwFpYbu7x/hvN1pvZ7HC5BlgCDGiy28XAAxZ4A+gpqX/bYTvnXMcpi0WpbzDe/2B3ukNJibZKBMMkzSf49X9UuEz4+Mj2PomkcuBEYHqTTQOANXGP14br1jc5fiIwEWDw4MHtfVrnnEuKxtnJNmzbtfd20mzSViI45lCfQFJ34E/ALWa27WDOYWaTgckAo0aNyt6Rn5xznVJj7+Kqml1Aj/QGkwKtJgIzWxX/WFIf4AxgtZnNauvkkgoIksDDZvbnZnZZBwyKezwwXOecc53Gvt7F2XnnUFu3jz4laXi43B9YSHC30IOSbmnjWAH3AkvM7I4WdpsKXB3ePTQW2Gpm61vY1znn0qJv98bxhrLzzqG2qoaOMLOF4fJ/AM+Z2dXhXUCvAj9v5dgPA58FFkiaG677FjAYwMx+DTwDXAAsB3aEz+Gcc51KYX6EPt0Ks7ZE0FYiiB+AezzwWwjuApLU0NqBZvZv2rjF1IKZHr7Yjjidcy6tSrO4L0FbiWCNpC8R3M0zEvgHgKQuQEGKY3POuU6jLFaUtXMStNWP4DrgOOBa4Aoz2xKuHwv8PoVxOedcpxKMN5SDVUNmVgV8vpn104BpqQrKOec6m7JYERu311JX30B+XnaN19lqIpA0tbXtZnZRcsNxzrnOqTQWxQw2bt9Nvx7RdIeTVG21EZxC0PN3CkGv4OwbZMM559ohfqayXEsE/YBzgCuBTwNPA1PMbFGqA3POuc5k39zF2ddg3GpFl5nVm9k/zOwaggbi5cCLPheBcy7X7C0RZOG8BG2VCJBUBHyMoFRQDtwFPJnasJxzrnPp062QiKA6C0sEbTUWPwAMJ+gB/N24XsbOOZdT8vMi9O1elJW3kLZVIvgM8AFwMzApGD4ICBqNzcxiKYzNOec6lbJYNCs7lbXVjyC7bpZ1zrlDUBYrYt2W7EsE/kXvnHPtlK3jDXkicM65diorjvL+B7vZXdfqmJsZxxOBc861U2Nfgurt2dVg7InAOefaKb53cTbxROCcc+1U2jh3sSeC9pF0n6QqSc32PZA0TtJWSXPDv2+nKhbnnEuGbJ27uM2exYfgfuBu4IFW9nnFzC5MYQzOOZc0vbsWkh+RVw21l5m9DGxK1fmdc66jRSKitDj7ehenu43gFEnzJP1d0nFpjsU559pUGotSlWW9i9OZCGYDh5vZCcAvgb+0tKOkiZIqJVVWV1d3WIDOOddUWazIq4aSxcy2mdn2cPkZoEBS3xb2nWxmo8xsVElJSYfG6Zxz8UqzcO7itCUCSf0UjmInaUwYy/vpisc559qjLFbE1p172LWnPt2hJE3K7hqSNAUYB/SVtBb4DlAAYGa/Bi4FbpRUB+wEJpiZpSoe55xLhtLwFtKqbbUM7tM1zdEkR8oSgZld2cb2uwluL3XOuYyxb6ayXVmTCNJ915BzzmWUbJy72BOBc84loKw4+3oXeyJwzrkE9OxaQGFeJKvGG/JE4JxzCZBEaZb1JfBE4JxzCSqLZVdfAk8EzjmXoLJYUYk5lT4AABHsSURBVFZNYu+JwDnnElRaHKXaSwTOOZe7ymJRamrr+KC2Lt2hJIUnAuecS1BjX4KqmuwoFXgicM65BGXb3MWeCJxzLkHZ1rvYE4FzziUofuC5bOCJwDnnElRclE+XgjwvETjnXK6SFPYl8BKBc87lrNJY1EsEzjmXy8pi0awZeM4TgXPOHYSy4iI2bKslGyZWTFkikHSfpCpJC1vYLkl3SVouab6kkamKxTnnkq0sFmXnnnpqsqB3cSpLBPcD57ey/aNARfg3EfhVCmNxzrmkKm3sXZwF1UMpSwRm9jKwqZVdLgYesMAbQE9J/VMVj3POJVNpFs1Uls42ggHAmrjHa8N1B5A0UVKlpMrq6uoOCc4551qTTb2LM6Kx2Mwmm9koMxtVUlKS7nCcc25v72IvERyadcCguMcDw3XOOdfpdS/Kp3tRvpcIDtFU4Orw7qGxwFYzW5/GeJxzLiGlsSKqsmCmsvxUnVjSFGAc0FfSWuA7QAGAmf0aeAa4AFgO7AD+I1WxOOdcKpQVZ8fcxSlLBGZ2ZRvbDfhiqp7fOedSrSxWROWqzekO45BlRGOxc851RsEwE5nfu9gTgXPOHaTSWJTd9Q1s2bEn3aEcEk8Ezjl3kPb2JcjwBmNPBM45d5DKsqQvgScC55w7SGXFjVNWeonAOedy0t6B5zJ8pjJPBM45d5CiBXn06FKQ8b2LPRE459whKIsVeSJwzrlcVhbL/N7Fngicc+4QlBZn/tzFngicc+4QlMWKqKqppaEhc3sXeyJwzrlDUBaLUtdgbNqxO92hHDRPBM45dwiyYaYyTwTOOXcIGmcqq8rgBmNPBM45dwj2DTPhJQLnnMtJJd0bq4a8ROCcczmpMD9C726FGT0CaUoTgaTzJb0labmkbzSz/VpJ1ZLmhn/XpzIe55xLhdLioozuS5DKOYvzgP8HnAOsBWZKmmpmi5vs+piZ3ZSqOJxzLtUyvXdxKksEY4DlZrbCzHYDjwIXp/D5nHMuLTJ9vKFUJoIBwJq4x2vDdU1dImm+pCckDWruRJImSqqUVFldXZ2KWJ1z7qCVxaJs3F5LXX1DukM5KOluLP4bUG5mxwPPAX9obiczm2xmo8xsVElJSYcG6JxzbSmNRWkweP+DzOxdnMpEsA6I/4U/MFy3l5m9b2aNFWu/A05KYTzOOZcSZcWZ3bs4lYlgJlAh6QhJhcAEYGr8DpL6xz28CFiSwniccy4lMn3u4pTdNWRmdZJuAp4F8oD7zGyRpO8BlWY2FZgk6SKgDtgEXJuqeJxzLlUyvXdxyhIBgJk9AzzTZN2345a/CXwzlTE451yq9e1eiJS5k9inu7HYOecyXn5ehL7dizK2asgTgXPOJUFZrChjh5nwROCcc0lQVhzN2KGoPRE451wSlMaiVHmJwDnncldZrIiN23ezJwN7F3sicM65JGi8hbS6JvOqhzwROOdcEmTy3MUp7UfgnHO54IJfvMLi9dsA+OQ9r+1df2z/GM/cfHq6wmo3LxE459whGjm4JwV52m9dQZ4YeXivNEWUGE8Ezjl3iCaNryCi/RNBnsSk8UenKaLEeCJwzrlDVBqLctlJA2ksFAi4aMRhlBZH0xpXe3kicM65JJg0voL8vOAr1YDKlZtYv3VneoNqJ08EzjmXBI2lAgk+ckwpVTW7+dQ9r7FsQ026Q2uTJwLnnEuSSeMrGF3em//71Id47HNjqWswLv3168xcuSndobXKE4FzziVJaSzK4587hdLiKMcd1oM/33gqfboV8pnfTefZRe+lO7wWeSJwzrkUGdS7K0/ceCrH9I9x40OzeOiNVekOqVmeCJxzLoV6dyvkkRtOZtzQUm79y0Lu+OdbmFm6w9qPUhmQpPOBXxBMVfk7M/tRk+1FwAMEk9a/D1xhZitbO+eoUaOssrIyNQE751yK1NU38K0nF/B45dpmt7fUCzm+13J79m+JpFlmNqq5bSkrEUjKA/4f8FHgWOBKScc22e06YLOZHQ3cCfw4VfE451w65edF+PElxzP8sNgB21rrhdwRvZZTOdbQGGC5ma0AkPQocDGwOG6fi4HbwuUngLslyTpbuck555JAEvddO5pTf/QCdQ37vubq6o1Xl1Vzzh0vHXBMXX0DdfX7fyUmu9dyKhPBAGBN3OO1wMkt7WNmdZK2An2AjfE7SZoITAQYPHhwquJ1zrmUK41FmTB6EFNmrqG+wRAwqHcXjmmmpNCo3ow1m3ZiBKWBS0cNSmqv5YwYfdTMJgOTIWgjSHM4zjl3SCaNr+CPs9ZS32AU5Ud44sZTW/1ir9q2i9Nvn0ZtXUNKxjBK5V1D64BBcY8Hhuua3UdSPtCDoNHYOeeyVnwv5Pb8uk90/0SlMhHMBCokHSGpEJgATG2yz1TgmnD5UuAFbx9wzuWCxl7I7f11n+j+iUj17aMXAD8nuH30PjP7gaTvAZVmNlVSFHgQOBHYBExobFxuid8+6pxziWvt9tGUthGY2TPAM03WfTtueRdwWSpjcM451zrvWeyccznOE4FzzuU4TwTOOZfjPBE451yOS+ldQ6kgqRpIZCzXvjTpqZyF/Bqzg19jduis13i4mZU0tyHjEkGiJFW2dMtUtvBrzA5+jdkhE6/Rq4accy7HeSJwzrkclwuJYHK6A+gAfo3Zwa8xO2TcNWZ9G4FzzrnW5UKJwDnnXCs8ETjnXI7L2kQg6XxJb0laLukb6Y4nFSStlLRA0lxJWTEkq6T7JFVJWhi3rrek5yQtC/9N3mStadDCNd4maV34Xs4NR+7NWJIGSZomabGkRZJuDtdnzXvZyjVm3HuZlW0EkvKApcA5BFNkzgSuNLPFrR6YYSStBEaZWWfsvHJQJJ0BbAceMLPh4brbgU1m9qMwqfcys6+nM85D0cI13gZsN7OfpjO2ZJHUH+hvZrMlFQOzgE8A15Il72Ur13g5GfZeZmuJYAyw3MxWmNlu4FHg4jTH5NrBzF4mmJsi3sXAH8LlPxD8Z8tYLVxjVjGz9WY2O1yuAZYQzFGeNe9lK9eYcbI1EQwA1sQ9XkuGvkFtMOCfkmZJmpjuYFKozMzWh8vvAWXpDCaFbpI0P6w6ytgqk6YklRNMPjWdLH0vm1wjZNh7ma2JIFecZmYjgY8CXwyrHLJaOJVp9tVnwq+Ao4ARwHrgZ+kNJzkkdQf+BNxiZtvit2XLe9nMNWbce5mtiWAdMCju8cBwXVYxs3Xhv1XAkwRVYtloQ1gf21gvW5XmeJLOzDaYWb2ZNQC/JQveS0kFBF+QD5vZn8PVWfVeNneNmfheZmsimAlUSDpCUiEwAZia5piSSlK3sIEKSd2Ac4GFrR+VsaYC14TL1wB/TWMsKdH45Rj6JBn+XkoScC+wxMzuiNuUNe9lS9eYie9lVt41BBDesvVzIA+4z8x+kOaQkkrSkQSlAAjmnn4kG65R0hRgHMFQvhuA7wB/AR4HBhMMQX65mWVsY2sL1ziOoCrBgJXA5+Lq0jOOpNOAV4AFQEO4+lsEdehZ8V62co1XkmHvZdYmAuecc+2TrVVDzjnn2skTgXPO5ThPBM45l+M8ETjnXI7zROCccznOE0EnIMkk/Szu8dfCQciSce77JV2ajHO18TyXSVoiadohnON3ko49yGNfO4TnfVFSRk02fjAkfSKR11fSKEl3pTKmjnAon41c4Ymgc6gFPiWpb7oDiScpP4HdrwNuMLOzDvK58szs+oMdIdbMTj2Y43LMJ4B2JwIzqzSzSQf7ZOEowActwc9fi/yz0TZPBJ1DHcE8p19uuqHpL3pJ28N/x0l6SdJfJa2Q9CNJV0maEc5RcFTcaT4iqVLSUkkXhsfnSfqJpJnh4FifizvvK5KmAgd8KUu6Mjz/Qkk/Dtd9GzgNuFfST5rsP07Sy5KeVjA/xK8lRRqvRdLPJM0DTon/ZR5u+4GkeZLekFQWri+T9GS4fp6kU5t5XVp6vl+Fr8MiSd9t602RNFrSa+HzzJBULCkq6ffhazBH0lnhvtdK+ouCMfZXSrpJ0lfCfd6Q1Dvc70VJv1AwTv1CSWPC9b3D4+eH+x8frr9NwcBlL4bv86S4+D4TxjVX0m8av3ibe+3C1+ki4Cfh/kdJmqRgLP35kh5t5vrHSXqqrTiaHNP0PW0pxuvCz+MMSb+VdHe4/v7wPZsO3B7G+Q8FAyu+ImlYuN9l4es3T9LL4brj4p5rvqSKJp8NKfjMLwzfvyvirvNFSU9IelPSw5LU1ucjq5iZ/6X5j2Bs+hhBL8QewNeA28Jt9wOXxu8b/jsO2AL0B4oIxlL6brjtZuDnccf/gyDpVxCMxBoFJgK3hvsUAZXAEeF5PwCOaCbOw4DVQAlBb+YXgE+E214kmBuh6THjgF3AkQS9vJ9rvB6CnpeXx+279xzhto+Hy7fHxfoYweBehOfr0czr0tLz9Y477kXg+JZiBwqBFcDo8HEsvOavEvRUBxgWvh5RgnH2lwPF4euzFfh8uN+dcTG/CPw2XD4DWBgu/xL4Trh8NjA3XL4NeC18j/oC7wMFwDHA34CCcL97gKvbeO3uZ//P0rtAUbjcs4X37qnW4mjmmL3vaUsxEnyOVgK9w2t5Bbg7LsangLzw8fNARbh8MvBCuLwAGBAfe/gaXhX3/nVp8tm4hODzkEcw6ulqgv8/48L3ayDB/5PXCQZ0TPt3Q0f9eYmgk7Bg1MIHgESK4jMtGBO9Fngb+Ge4fgFQHrff42bWYGbLCL7chhGMTXS1pLkE3f77ECQKgBlm9k4zzzcaeNHMqs2sDniY4MusLTMsmBuiHphCUHoAqCcYsKs5uwm+ECCY8KPxes4mGN0RCwb22prA810uaTYwBziO1qtJhgLrzWxm+Fzbwms+DXgoXPcmwTAJQ8JjpplZjZlVE3yx/C1c3/T9mBIe/zIQk9QzPO+D4foXgD6SYuH+T5tZrQUTEFURfImNB04CZobv4XiC5Nfaa9fUfOBhSZ8hKJW2pbk4mop/T1uKcQzwkpltMrM9wB+bnOOPZlavYFTPU4E/hsf/huCLG+BV4H5JNxB8sUPwBf4tSV8HDjeznU3OexowJfzcbABeIvhMQ/CZWWvBQHFzafk1y0pJqYNzSfNzYDbw+7h1dYRVeGEVR2Hcttq45Ya4xw3s/942HUfEAAFfMrNn4zdIGkdQIkim5p4fYFf4Zd2cPRb+jCP4cknks3rA80k6gqCkNdrMNku6n+CXfDIdyvvR3vM2vhYC/mBm32xm//a+dh8jSOQfB/5H0ofCZJdIHE3Fv6fNxiiprcloGj9/EWCLmY1ouoOZfV7SyeE1zJJ0kpk9ElYpfQx4RtLnwqTaHu25tqzlJYJOxILBtx4naHhttJLgVxUEdbwFB3HqyyRFFLQbHAm8BTwL3KhgGF0kDVEwimlrZgBnSuob1vVeSfCrqi1jFIwEGwGuAP59ENfQ6HngxjDmPEk92vl8MYIvmK0K2hs+2sbzvAX0lzQ6fK5iBY2XrwBXheuGEAye9laC19BYN30asDUs1cSfdxyw0ZqM39/E88ClkkrDY3pLOryN560hqLpq/FExyMymAV8nqJLsnuB1tKWlGGcSfI56ha/pJc0dHF7/O5IuC4+XpBPC5aPMbLqZfRuoBgYpGIhxhZndRTCq6fFNTvkKcEX4uSkhSIIzknzNGckTQefzM4I62Ea/JfhPMw84hYP7tb6a4AP/d4J6613A7wgag2crmET9N7TxK8iCERS/AUwD5gGzzKw9wwjPBO4mmMrvHfaNmnowbgbOkrSAoNqjueqdA57PzOYRVAm9CTxCULXQIgumOL0C+GX42j9HUIK4B4iEz/8YcG1YNZeIXZLmAL9mX9K/DThJ0nzgR+wbqrml+BYDtxLMUDc/jK9/a8cQTNn6X+FzVwAPhdcxB7jLzLYkeB2tailGC+bR+D+Cz+SrBD92mqvigyA5Xhe+B4vYN+XsT8IG34UEbRfzCOYKXhhWIw0nqGqN9yRBddg8gvat/zaz95JxrZnORx91KRX+uv2amV2Yjc+XKEkvEsRXme5Y0klSdzPbHpYIniRogD+UHwjuEHiJwDmXDreFv9wXEpTa/pLmeHKalwiccy7HeYnAOedynCcC55zLcZ4InHMux3kicM65HOeJwDnnctz/ByiEaCdeFeHuAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSlO63eCwgfI",
        "outputId": "fe20cb03-a4a2-44b6-c0c8-bb165f901fa2"
      },
      "source": [
        "np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 33.62,  51.63,  61.6 ,  68.41,  74.32,  78.82,  82.53,  85.47,\n",
              "        88.3 ,  90.85,  93.11,  94.75,  95.88,  96.83,  97.71,  98.42,\n",
              "        98.89,  99.3 ,  99.62,  99.77,  99.9 , 100.  , 100.  , 100.  ,\n",
              "       100.  , 100.  , 100.  ])"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "pWn6p2TFwk14",
        "outputId": "59769cc9-be50-44ed-811d-5547027c8bf9"
      },
      "source": [
        "pca2 = PCA()\n",
        "\n",
        "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1)\n",
        "\n",
        "X_reduced_train = pca2.fit_transform(scale(X_train))\n",
        "n = len(X_reduced_train)\n",
        "\n",
        "kf_10 = KFold( n_splits=10, shuffle=True, random_state=1)\n",
        "\n",
        "mse = []\n",
        "\n",
        "score = -1*cross_val_score(regr, np.ones((n,1)), y_train.ravel(), cv=kf_10, scoring='neg_mean_squared_error').mean()    \n",
        "mse.append(score)\n",
        "\n",
        "for i in np.arange(1, 28):\n",
        "    score = -1*cross_val_score(regr, X_reduced_train[:,:i], y_train.ravel(), cv=kf_10, scoring='neg_mean_squared_error').mean()\n",
        "    mse.append(score)\n",
        "\n",
        "plt.plot(np.array(mse), '-v')\n",
        "plt.xlabel('Number of principal components in regression')\n",
        "plt.ylabel('MSE')\n",
        "plt.title('popother')\n",
        "plt.xlim(xmin=-1);"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MamBj_UHw5fY",
        "outputId": "ad5e58fd-0393-40f7-9dc5-de84c82f31c0"
      },
      "source": [
        "X_reduced_test = pca2.transform(scale(X_test))[:,:7]\n",
        "\n",
        "regr = LinearRegression()\n",
        "regr.fit(X_reduced_train[:,:7], y_train)\n",
        "\n",
        "pred = regr.predict(X_reduced_test)\n",
        "mean_squared_error(y_test, pred)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "450062199.44077724"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcMkDK_hxGtj"
      },
      "source": [
        "PLS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "ZjFMuhL_xGI4",
        "outputId": "2a821ce9-281c-451e-bb39-83f12f7a3702"
      },
      "source": [
        "n = len(X_train)\n",
        "\n",
        "kf_10 = KFold(n_splits=10, shuffle=True, random_state=1)\n",
        "\n",
        "mse = []\n",
        "\n",
        "for i in np.arange(1, 20):\n",
        "    pls = PLSRegression(n_components=i)\n",
        "    score = cross_val_score(pls, scale(X_train), y_train, cv=kf_10, scoring='neg_mean_squared_error').mean()\n",
        "    mse.append(-score)\n",
        "\n",
        "plt.plot(np.arange(1, 20), np.array(mse), '-v')\n",
        "plt.xlabel('Number of principal components in regression')\n",
        "plt.ylabel('MSE')\n",
        "plt.title('popother')\n",
        "plt.xlim(xmin=-1)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-1.0, 19.9)"
            ]
          },
          "metadata": {},
          "execution_count": 81
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IgrHVYgxaoP",
        "outputId": "f460add1-72e9-423e-cf6a-5152f9b92daf"
      },
      "source": [
        "pls = PLSRegression(n_components=2)\n",
        "pls.fit(scale(X_train), y_train)\n",
        "\n",
        "mean_squared_error(y_test, pls.predict(scale(X_test)))"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "457561596.47898674"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEDx-eOYR-G1"
      },
      "source": [
        "Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZJD2o-EUbLa7",
        "outputId": "d74ddf9d-fbfb-4b81-d9c5-ce4b7955cf88"
      },
      "source": [
        "from numpy import arange\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/selva86/datasets/master/midwest.csv'\n",
        "dataframe = read_csv(url, header=None)\n",
        "data = dataframe.values\n",
        "X, y = data[:, :-1], data[:, -1]\n",
        "\n",
        "model = ElasticNet()\n",
        "\n",
        "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\n",
        "grid = dict()\n",
        "grid['alpha'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.0, 1.0, 10.0, 100.0]\n",
        "grid['l1_ratio'] = arange(0, 1, 0.01)\n",
        "\n",
        "search = GridSearchCV(model, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
        "\n",
        "results = search.fit(X, y)\n",
        "\n",
        "print('MAE: %.3f' % results.best_score_)\n",
        "print('Config: %s' % results.best_params_)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "27000 fits failed out of a total of 27000.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "24300 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 681, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py\", line 943, in fit\n",
            "    y_numeric=True,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/base.py\", line 576, in _validate_data\n",
            "    X, y = check_X_y(X, y, **check_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 968, in check_X_y\n",
            "    estimator=estimator,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 738, in check_array\n",
            "    array = np.asarray(array, order=order, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py\", line 83, in asarray\n",
            "    return array(a, dtype, copy=False, order=order)\n",
            "ValueError: could not convert string to float: 'inmetro'\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "2700 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 681, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py\", line 943, in fit\n",
            "    y_numeric=True,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/base.py\", line 576, in _validate_data\n",
            "    X, y = check_X_y(X, y, **check_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 968, in check_X_y\n",
            "    estimator=estimator,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 738, in check_array\n",
            "    array = np.asarray(array, order=order, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py\", line 83, in asarray\n",
            "    return array(a, dtype, copy=False, order=order)\n",
            "ValueError: could not convert string to float: 'WI'\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
            "  category=UserWarning,\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-0cd0663273dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0msearch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'neg_mean_absolute_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# perform the search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;31m# summarize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MAE: %.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    924\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    941\u001b[0m                 \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_copied\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m                 \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m                 \u001b[0my_numeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m             )\n\u001b[1;32m    945\u001b[0m             y = check_array(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    574\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m         \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m     )\n\u001b[1;32m    970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    736\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    739\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m                 raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'PID'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGKUtz03worR"
      },
      "source": [
        "Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "kYGUV9plwoEN",
        "outputId": "a9bee05b-944c-46d3-a91d-eb383ae38860"
      },
      "source": [
        "import pandas\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import ElasticNet\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/selva86/datasets/master/midwest.csv\"\n",
        "names = [\"PID\",\"county\",\"state\",\"area\",\"poptotal\",\"popdensity\",\"popwhite\",\"popblack\",\n",
        "\"popamerindian\",\"popasian\",\"popother\",\"percwhite\",\"percblack\",\"percamerindan\",\"percasian\",\n",
        "\"percother\",\"popadults\",\"perchsd\",\"percollege\",\"percprof\",\"poppovertyknown\",\"percpovertyknown\",\n",
        "\"percbelowpoverty\",\"percchildbelowpovert\",\"percadultpoverty\",\"percelderlypoverty\",\"inmetro\",\"category\"]\n",
        "dataframe = pandas.read_csv(url, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "\n",
        "seed = 7\n",
        "\n",
        "models = []\n",
        "models.append(('LR', LinearRegression()))\n",
        "models.append(('R', Ridge()))\n",
        "models.append(('LA', Lasso()))\n",
        "models.append(('EN', ElasticNet()))\n",
        "models.append(('PLT', plt()))\n",
        "models.append(('PCA', PCA()))\n",
        "\n",
        "results = []\n",
        "names = []\n",
        "scoring = 'accuracy'\n",
        "for name, model in models:\n",
        "\tkfold = model_selection.KFold(n_splits=10)\n",
        "\tcv_results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
        "\tresults.append(cv_results)\n",
        "\tnames.append(name)\n",
        "\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "\tprint(msg)\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Algorithm Comparison')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.show()"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-4650cde14247>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LA'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLasso\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'EN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mElasticNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PLT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PCA'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# evaluate each model in turn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
          ]
        }
      ]
    }
  ]
}